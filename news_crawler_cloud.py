import streamlit as st
import pandas as pd
import datetime as dt
from datetime import datetime
import pygsheets
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import urllib.request as req
import bs4
import json
import gspread
from google.oauth2.service_account import Credentials
from openai import OpenAI
from pandas.tseries.offsets import BusinessDay

# --- è¨­å®šå€ ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

# é—œéµå­—æ¸…å–® (å»¶ç”¨ä½ çš„ç‰ˆæœ¬)
KEYWORDS = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
TITLE_KEYWORDS = ["å°æ°´åŠ›","å…‰é›»","ç¶ èƒ½","ç¶ é›»","é¢¨èƒ½","å¤ªé™½èƒ½","å†ç”Ÿ","å„²èƒ½","æ¸›ç¢³","ESG","é›»æ± ","åœ°ç†±","é¢¨åŠ›","ç™¼é›»","é­šå¡­","åœŸåœ°","æ°´åŠ›","æ·¨é›¶","æ¼é›»","å…‰å„²","ä½åœ°åŠ›","å”®é›»","å°é›»","é…é›»","è¼¸é›»","å‡å£“","ç’°ç¤¾","ç”¨é›»å¤§æˆ¶","é¥‹ç·š","é›»è¡¨","è¡¨å‰","è¡¨å¾Œ","éœ€é‡åæ‡‰","é›»ç¶²","åœŸåœ°é–‹ç™¼","é›»å» ","å‚™è½‰","èª¿é »","PCS","EMS","BMS","é›»åŠ›äº¤æ˜“","ä½µç¶²","ç±Œè¨­","é¢¨é›»","é›»åƒ¹","é›»æ¥­","é¦™å¤¾è˜­","è¾²æ¥­è£œåŠ©","CPPA","è¾²é›»","è¾²æ¥­è¨­æ–½è¨±å¯","æ²¼æ°£","ç”Ÿè³ªèƒ½","Solar","PV","energy","solar","storage","å…‰ä¼","èƒ½æºæ”¿ç­–","ç¢³æ¬Š","ç¢³è²»","èº‰è³¼","èƒ½æºç½²","é›»æ¥­æ³•","èº‰è³¼è²»ç‡","æ¼é›»å…±ç”Ÿ"]
COMPANY_KEYWORDS = ["éº—å‡", "é™½å…‰ä¼ç‰¹å®¶é›»åŠ›" ,"é™½å…‰ä¼ç‰¹å®¶" ,"å°æ±½é›»ç¶ èƒ½" ,"å°æ±½é›»" ,"å¯Œå¨é›»åŠ›" ,"å¯Œå¨" ,"ç“¦ç‰¹å…ˆç”Ÿ" ,"å—æ–¹é›»åŠ›" ,"çŸ³é–€å±±æ–°é›»åŠ›" ,"å¥‡ç•°æœæ–°èƒ½æº" ,"é¦–ç¾ç¶ èƒ½" ,"é¦–ç¾" ,"ä¸‰åœ°æ€ªç¸é›»åŠ›" ,"ä¸‰åœ°æ€ªç¸" ,"æ¨ºéŠ³ç¶ é›»ç§‘æŠ€" ,"æ¨ºéŠ³ç¶ é›»" ,"æ˜Ÿæ˜Ÿé›»åŠ›" ,"æ˜Ÿæ˜Ÿ" ,"å¤©èƒ½ç¶ é›»" ,"é–‹é™½é›»åŠ›" ,"é–‹é™½" ,"åšæ›œé›»åŠ›" ,"åšæ›œ" ,"äºç¦å„²èƒ½" ,"è«æ¯”ç¶ é›»" ,"è¯åŸèƒ½æº" ,"è¯åŸ" ,"åç«£ç¶ èƒ½" ,"åç«£" ,"å¤§åŒæ™ºèƒ½" ,"å¤ªé™½ç¥é›»åŠ›" ,"å¤ªé™½ç¥" ,"å¤§è‡ªç„¶èƒ½æºé›»æ¥­" ,"å¯¶å¯Œé›»åŠ›" ,"å¯¶å¯Œ" ,"ä¸­æ›œ" ,"é˜¿æ³¢ç¾…é›»åŠ›" ,"é˜¿æ³¢ç¾…" ,"ç“¦åŠ›é›»èƒ½" ,"é™½å…‰ç¶ é›»" ,"çºŒèˆˆ" ,"èƒ½å…ƒè¶…å•†" ,"å°ç£ç¢³è³‡ç”¢é›»æ¥­" ,"åº·å±•é›»åŠ›" ,"åº·å±•" ,"å°åŒ–ç¶ èƒ½" ,"å°åŒ–" ,"ä¸Šæ™Ÿèƒ½æºç§‘æŠ€" ,"ä¸Šæ™Ÿèƒ½æº" ,"æ™¨æ˜Ÿé›»åŠ›" ,"æ™¨æ˜Ÿ" ,"å‚‘å‚…èƒ½æº" ,"å‚‘å‚…" ,"è©®å¯¦èƒ½æº" ,"è©®å¯¦" ,"å¯¶å³¶é™½å…‰é›»åŠ›äº‹æ¥­" ,"èª æ–°é›»åŠ›" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"é›²è±¹èƒ½æº" ,"é¦™å°æ°¸çºŒ" ,"ç¾©é›»æ™ºæ…§èƒ½æº" ,"ç¾©é›»æ™ºæ…§" ,"å®‡è»’é›»æ¥­" ,"ç–æš‰æ°¸çºŒé›»èƒ½" ,"æ›œè¶Šç¶ é›»" ,"è‰¾æ¶…çˆ¾é›»åŠ›" ,"è‰¾æ¶…çˆ¾" ,"èˆˆæ—ºèƒ½æº" ,"èˆˆæ—º" ,"èŒ‚æ¬£èƒ½æº" ,"èŒ‚æ¬£" ,"å’ŒåŒèƒ½æº" ,"å’ŒåŒ" ,"å®‰ç‘Ÿæ¨‚å¨" ,"ä¸Šé›†èƒ½æº" ,"å’Œæ½¤é›»åŠ›" ,"å’Œæ½¤" ,"æ¾æ¹–ç¶ é›»" ,"ç¦¾ä¸°é›»åŠ›" ,"ç¦¾ä¸°" ,"æ–°é‘«é›»åŠ›" ,"æ–°é‘«" ,"å°é”èƒ½æº" ,"å°é”" ,"ç²¾è¯èƒ½æº" ,"ç²¾è¯" ,"åœ‹ç¢©èƒ½æº" ,"åœ‹ç¢©" ,"æ°¸é¤˜æ™ºèƒ½" ,"æ†åˆ©é›»èƒ½" ,"è‰¾åœ°é›»åŠ›" ,"è‰¾åœ°" ,"æ–°æ™¶å¤ªé™½å…‰é›»ç§‘æŠ€" ,"æ–°æ™¶å¤ªé™½å…‰é›»" ,"å¤©å‹¢èƒ½æº" ,"å¤©å‹¢" ,"æ‰¿ç ”èƒ½æºç§‘æŠ€" ,"æ‰¿ç ”èƒ½æº" ,"çµ±ç›Šèƒ½æº" ,"çµ±ç›Š" ,"æ€¡å’Œç¶ é›»è¶…å•†" ,"ä¸­è¯ç³»çµ±æ•´åˆ" ,"è£•é´»èƒ½æº" ,"è£•é´»" ,"æ˜å¾½é›»åŠ›" ,"æ˜å¾½" ,"å¼˜æ˜Œæ³°" ,"æ˜¶å³°ç¶ èƒ½ç§‘æŠ€" ,"æ˜¶å³°ç¶ èƒ½" ,"æˆç¶ èƒ½" ,"æœ‰æˆ" ,"åè¬ä¼ç‰¹é›»åŠ›" ,"åè¬ä¼ç‰¹" ,"å‹é”é›»åŠ›" ,"å‹é”" ,"æ¾¤ç”Ÿèƒ½æº" ,"æ¾¤ç”Ÿ" ,"å…‰åˆä½œç”¨" ,"æ˜•æ˜é›»åŠ›" ,"æ˜•æ˜" ,"é´»æ™¶æ–°ç§‘æŠ€" ,"é´»æ™¶æ–°" ,"æ¯“ç›ˆ" ,"å¤©éº‹é›»åŠ›" ,"å¤©éº‹" ,"æ–°å…‰æºé›»åŠ›" ,"æ–°å…‰æº" ,"æ†ç«‹èƒ½æº" ,"æ†ç«‹" ,"æ˜Ÿè¾°é›»åŠ›" ,"æ˜Ÿè¾°" ,"è¾°æ˜‡èƒ½æº" ,"è¾°æ˜‡" ,"åº·èª èƒ½æº" ,"åº·èª " ,"å¯¬åŸŸèƒ½æº" ,"å¯¬åŸŸ" ,"å¤§å‰µé›»åŠ›" ,"å¤§å‰µ" ,"å¤ªå‰µèƒ½æº" ,"å¤ªå‰µ" ,"å¤§çŒ©çŒ©é›»èƒ½äº¤æ˜“" ,"å¥‰å¤©é›»åŠ›" ,"å°ç£å¨è¿ªå…‹è‰¾å…§æ–¯é”èƒ½æº" ,"è‚²æˆé›»åŠ›" ,"æ©™é‘«é›»åŠ›" ,"æ©™é‘«" ,"è€€é¼è³‡æºå¾ªç’°" ,"ä¸­æ—¥é›»åŠ›" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"å°ç£æ™ºèƒ½æ¼é›»ç§‘æŠ€" ,"æµ·åˆ©æ™®æ–°èƒ½æº" ,"æµ·åˆ©æ™®" ,"ç‰¹èˆˆèƒ½æºé¡§å•" ,"å°ç£æ™ºæ…§é›»èƒ½" ,"è¯æ—­èƒ½æºé–‹ç™¼" ,"éŒ¦æŒ¯èƒ½æº" ,"éŒ¦æŒ¯" ,"å®‰èƒ½é›»æ¥­" ,"å®‰èƒ½é›»æ¥­" ,"é‡‘è±¬èƒ½æºç§‘æŠ€" ,"é‡‘è±¬èƒ½æº" ,"å°å¡‘ç¶ é›»" ,"è¯ç’½èƒ½æº" ,"è¯ç’½" ,"è‚²æ¸²æŠ•è³‡" ,"æ­æ‚…èƒ½æº" ,"æ­æ‚…" ,"åº­æ—" ,"æ™Ÿé‹ç§‘æŠ€" ,"æ˜Ÿå´´é›»åŠ›" ,"æ˜Ÿå´´" ,"æ¼¢ç‚ºç§‘æŠ€å·¥ç¨‹" ,"ç«‹è±å…‰èƒ½" ,"ç«‹è±å…‰èƒ½" ,"ç‰ç’ƒå…‰ç¶ èƒ½" ,"ç‰ç’ƒå…‰" ,"é“é”çˆ¾èƒ½æº" ,"æ±æ³°ç¶ èƒ½æŠ•è³‡" ,"å¯Œé™½èƒ½é–‹ç™¼" ,"å‰ç¥¥ç§‘æŠ€" ,"å‰ç¥¥" ,"å‡±æ™ºç¶ èƒ½ç§‘æŠ€" ,"æ°¸è±å¤ªé™½èƒ½èƒ½æº" ,"è·¯åŠ å¤ªé™½èƒ½æŠ•è³‡é¡§å•" ,"å¦‚æ™…ç¶ èƒ½é–‹ç™¼" ,"åŠ›å±±ç¶ èƒ½ç§‘æŠ€" ,"æ±ä¹‹å„„ç¶ èƒ½" ,"è¯å®èšèƒ½ç§‘æŠ€" ,"å¤ªèƒ½ç³»çµ±" ,"æ˜“æ™¶ç¶ èƒ½ç³»çµ±" ,"æ°¸æ»”ç¶ èƒ½" ,"æ°¸æ»”" ,"å°ç£æ‰€æ¨‚å¤ªé™½èƒ½ç§‘æŠ€" ,"ç¿°å¯èƒ½æº" ,"ç¿°å¯" ,"å’Œåˆè³‡æºç¶ èƒ½" ,"ç¶­çŸ¥ç§‘æŠ€" ,"åŠ é›²è¯ç¶²" ,"æ±æ­¦é›»æ©Ÿå·¥æ¥­" ,"å‰é€²ç¶ èƒ½ç§‘æŠ€" ,"å…‰æ—­ç›ˆç§‘æŠ€" ,"å…‰æ—­ç›ˆ" ,"æ™´æ£ å¯¬èƒ½æºå·¥ç¨‹" ,"å‡±ç±³å…‹å¯¦æ¥­" ,"å¤§æ—¥é ­" ,"æ–°æ™¶å…‰é›»" ,"æ†åˆ©èƒ½æº" ,"å…‰é¼èƒ½æºç§‘æŠ€" ,"ç’°äºå…‰é›»" ,"å®£å† " ,"è¡†å´´èƒ½æº" ,"è¡†å´´" ,"æ¨‚é™½èƒ½æº" ,"å°ç£å’Œæš„ç¶ èƒ½" ,"è–å±•å…‰èƒ½" ,"å‰µç¿èƒ½æº" ,"å‰µç¿" ,"ç™¾åˆ©å¯Œèƒ½æº" ,"ç™¾åˆ©å¯Œ" ,"é‡‘é›»ç™¼èƒ½æº" ,"é¼æ‰¿èƒ½æºç§‘æŠ€" ,"æ˜¶è€€é–‹ç™¼" ,"æ˜Ÿèƒ½" ,"æ—¥å‹å†ç”Ÿèƒ½æº" ,"åœ‹è»’ç§‘æŠ€" ,"åœ‹è»’" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"æ˜‡éˆºå…‰é›»" ,"æ˜‡éˆºå…‰é›»" ,"ç¶ é †ç§‘æŠ€" ,"ç¶ é †" ,"è£•é›»èƒ½æº" ,"è£•é›»" ,"æš˜å…‰ç¶ èƒ½å¯¦æ¥­" ,"å‡¡å±•ç¶ èƒ½ç§‘æŠ€" ,"æ—­èª ç¶ èƒ½" ,"å¤§ç€šé‹¼éµ" ,"ç¶ è‘³èƒ½æºç§‘æŠ€" ,"ä¸­ç§Ÿé›»åŠ›ç§‘æŠ€" ,"æ­å¾—èƒ½æºå·¥ç¨‹" ,"å…‰ç…œèƒ½æº" ,"å…‰ç…œ" ,"æœæ—¥èƒ½æº" ,"å˜‰æ¯…é”å…‰é›»ä¼æ¥­" ,"å§‹å¾©èƒ½æº" ,"å§‹å¾©" ,"éŠ˜æ‡‹å·¥æ¥­" ,"å®‡è»’é‹¼éµå·¥ç¨‹" ,"æ™¶æˆèƒ½æº" ,"å…ƒæ™¶å¤ªé™½èƒ½ç§‘æŠ€" ,"å…†ä¿¡é›»é€šç§‘æŠ€" ,"ç™¾ç››èƒ½æºç§‘æŠ€" ,"ç™¾ç››èƒ½æº" ,"ç¦¾åŸæ–°èƒ½æºç§‘æŠ€" ,"æ—­å¤©èƒ½æº" ,"å…¨æ—¥å…‰" ,"é¨°æšç¶ é›»" ,"ç¶ è¾²é›»ç§‘" ,"è‡ºé¹½ç¶ èƒ½" ,"è‡ºé¹½" ,"æ˜•æ¯…ç§‘æŠ€" ,"æ½”åŠ›èƒ½æºäº‹æ¥­" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"é¦–ç¾èƒ½æº" ,"é¦–ç¾" ,"æ°¸æ—¥æ˜‡ç¶ èƒ½" ,"å¤çˆ¾ç‰¹æ‹‰å¤ªé™½èƒ½ç§‘æŠ€" ,"ç’°çƒå¤§å®‡å®™å¤ªé™½èƒ½å·¥æ¥­" ,"å‡Œç©æ‡‰ç”¨ç§‘æŠ€" ,"å‡Œç©æ‡‰ç”¨" ,"å´‘é¼ç¶ èƒ½ç’°ä¿" ,"ç››é½Šç¶ èƒ½" ,"ç››é½Š" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å—äºå…‰é›»" ,"å—äºå…‰é›»" ,"å®¶ç´³èƒ½æº" ,"å®¶ç´³" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½æœ‰é™å…¬å¸" ,"å£«èƒ½ç§‘æŠ€" ,"å£«èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"é—œéµæ‡‰ç”¨ç§‘æŠ€" ,"é—œéµæ‡‰ç”¨" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"å‘é™½å„ªèƒ½é›»åŠ›" ,"å‘é™½å„ªèƒ½" ,"ä¿¡é‚¦é›»å­" ,"ä¿¡é‚¦é›»å­" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å°ç£é”äº¨èƒ½æºç§‘æŠ€" ,"å°ç£é”äº¨èƒ½æº" ,"å¤©æ³°èƒ½æº" ,"å¤©æ³°" ,"æ³“ç­Œç§‘æŠ€" ,"æ³“ç­Œ" ,"æˆç²¾å¯†" ,"æœ‰æˆç²¾å¯†" ,"æ›œæ˜‡ç¶ èƒ½" ,"æ›œæ˜‡" ,"é‡‘é™½æ©Ÿé›»å·¥ç¨‹" ,"æ±å…ƒé›»æ©Ÿ" ,"æ±å…ƒé›»æ©Ÿ" ,"å…†æ´‹å¤ªé™½èƒ½æº" ,"å…†æ´‹å¤ªé™½èƒ½æºæœ‰é™å…¬å¸" ,"é‘«ç›ˆèƒ½æº" ,"é‘«ç›ˆ" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"æ˜è»’ç§‘æŠ€" ,"æ˜è»’ç§‘æŠ€æœ‰é™å…¬å¸" ,"ç´¹æ´²èˆˆæ¥­" ,"ç´¹æ´²èˆˆæ¥­" ,"åšç››å…‰é›»ç§‘æŠ€" ,"åšç››å…‰é›»ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ³“å¾·èƒ½æºç§‘æŠ€" ,"æ³“å¾·èƒ½æº" ,"ç¶ æºç§‘æŠ€" ,"ç¶ æº" ,"æ—¥å±±èƒ½æºç§‘æŠ€" ,"æ—¥å±±èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸"]
COMPANY_KEYWORDS = list(set([k.strip() for k in COMPANY_KEYWORDS if k.strip() != ""]))

# --- å·¥å…·å‡½å¼ ---
def get_pygsheets_wks():
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    gc = pygsheets.authorize(service_account_json=json.dumps(service_account_info))
    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA')
    return sh.worksheet_by_title('æœ€æ–°æ–°è')

def get_gspread_wks():
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    creds = Credentials.from_service_account_info(service_account_info, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    gc = gspread.authorize(creds)
    return gc.open_by_key("1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA").worksheet("æœ€æ–°æ–°è")

def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except: return ""

# --- å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±")
    
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²ä¸¦ä¸Šå‚³", use_container_width=True):
        with st.spinner("æ–°èçˆ¬å–ä¸­..."):
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # --- å»¶ç”¨ä½ çš„æ ¸å¿ƒçˆ¬èŸ²é‚è¼¯ ---
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []
            
            def append_news(title, url, date_obj, source, category):
                if start_date_obj <= date_obj <= end_date_obj:
                    m_title = [k for k in TITLE_KEYWORDS if k in title]
                    if m_title:
                        m_comp = [k for k in COMPANY_KEYWORDS if k in title]
                        dates.append(date_obj.strftime("%Y-%m-%d"))
                        sources.append(source)
                        categories.append(category)
                        title_keyword_matches.append(", ".join(m_title))
                        company_matches.append(", ".join(m_comp) if m_comp else "-")
                        titles.append(title)
                        links.append(url)

            # ğŸ” Yahoo (ä½ çš„é‚è¼¯)
            headers = {"User-Agent": "Mozilla/5.0"}
            for kw in KEYWORDS:
                try:
                    q = quote(kw)
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={q}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    for art in soup.select("li div[class*='Cf']"):
                        a_tag = art.find("a")
                        meta_div = art.find("div", class_="C(#959595)")
                        if not a_tag: continue
                        t = a_tag.text.strip()
                        l = a_tag["href"] if a_tag["href"].startswith("http") else f"https://tw.news.yahoo.com{a_tag['href']}"
                        d_obj = None
                        if meta_div:
                            time_str = meta_div.text.strip().split("â€¢")[-1].strip()
                            now = datetime.now()
                            if "å¤©å‰" in time_str: d_obj = now - dt.timedelta(days=int(time_str.replace("å¤©å‰","")))
                            elif "å°æ™‚" in time_str or "åˆ†é˜" in time_str: d_obj = now
                            else:
                                try: d_obj = datetime.strptime(time_str.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0], "%Y-%m-%d")
                                except: continue
                        if d_obj: append_news(t, l, d_obj, "Yahoo", kw)
                except: continue

            # ğŸ” UDN (ä½ çš„é‚è¼¯)
            for kw in KEYWORDS:
                try:
                    res = requests.get(f"https://udn.com/search/word/2/{quote(kw)}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    ti_box = soup.find("div", class_="context-box__content story-list__holder story-list__holder--full")
                    if not ti_box: continue
                    ti_h2 = ti_box.find_all("h2")
                    ti_time = ti_box.find_all("time", class_="story-list__time")
                    for l_idx, h2 in enumerate(ti_h2):
                        a = h2.find("a")
                        if a and l_idx < len(ti_time):
                            d_obj = datetime.strptime(ti_time[l_idx].text.strip()[:10], "%Y-%m-%d")
                            append_news(a.text.strip(), a["href"], d_obj, "UDN", kw)
                except: continue

            # --- çµ„åˆè³‡æ–™ ---
            final_df = pd.DataFrame({
                "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                "åŒ…å«æ¨™é¡Œé—œéµå­—": title_keyword_matches, "åŒ…å«å…¬å¸é—œéµå­—": company_matches,
                "æ¨™é¡Œ": titles, "æ–°èç¶²å€": links, "AI æ–°èæ‘˜è¦": [""] * len(titles)
            }).drop_duplicates(subset=["æ¨™é¡Œ"]).sort_values(by="æ—¥æœŸ", ascending=False)
            
            # --- å¯«å…¥ Google Sheet ---
            wks = get_pygsheets_wks()
            wks.clear(start='A1')
            wks.set_dataframe(final_df, 'A1')
            st.success(f"æ­¥é©Ÿä¸€å®Œæˆï¼æŠ“å–åˆ° {len(final_df)} ç­†ã€‚")

    st.divider()
    st.header("2ï¸âƒ£ äººå·¥å¯©æ ¸æ–‡ç« ")
    st.link_button("ğŸ“‚ é–‹å•Ÿ Sheets åˆªæ¸›", "https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA/edit", use_container_width=True)

    st.divider()
    st.header("3ï¸âƒ£ AI è‡ªå‹•æ‘˜è¦")
    if st.button("ğŸ¤– åŸ·è¡Œ OpenAI æ‘˜è¦", use_container_width=True):
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
        sheet = get_gspread_wks()
        rows = sheet.get_all_values()
        p = st.progress(0)
        for idx, row in enumerate(rows[1:], start=2):
            url = row[6] # æ–°èç¶²å€åœ¨ç¬¬ 7 æ¬„
            summary = row[7] if len(row) > 7 else ""
            if url.strip() and not summary.strip():
                st.write(f"æ‘˜è¦è™•ç†ä¸­: {url[:30]}...")
                text = extract_webpage_text(url)
                if text:
                    res = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å­—ï¼š\n\n{text[:2500]}"}]
                    )
                    sheet.update_cell(idx, 8, res.choices[0].message.content.strip())
            p.progress(idx / len(rows))
        st.success("æ­¥é©Ÿä¸‰å®Œæˆï¼")

    st.divider()
    st.header("4ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ğŸ“§ ç™¼é€é›»å­å ±", use_container_width=True):
        key = st.secrets.get("GAS_API_KEY", "")
        gas_url = f"https://script.google.com/macros/s/AKfycbwdJ3IukgLTY0MRVrmGiwRvw9OVW5CeSKaP98VrQsz5cG_1CE4ZAyLNODv3H_AU2n8h/exec?key={key}"
        if requests.get(gas_url).status_code == 200:
            st.balloons()
            st.success("éƒµä»¶ç™¼é€æˆåŠŸï¼")

# --- ä¸»ç•«é¢ ---
st.write("### ğŸ“„ ç›®å‰ Sheets ä¸­çš„æ–°èé è¦½")
try:
    wks = get_pygsheets_wks()
    st.dataframe(wks.get_as_df(), use_container_width=True)
except:
    st.info("å°šæœªé€£ç·šåˆ° Sheetsã€‚")

import streamlit as st
import pandas as pd
import datetime as dt
from datetime import datetime
import pygsheets
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import urllib.request as req
import time as tt
import json

# --- 1. ç¶²é ä»‹é¢è¨­å®š ---
st.set_page_config(page_title="æ–°èç›£æ¸¬ç³»çµ±", page_icon="ğŸ“°")
st.title("ğŸ“° ç¶ èƒ½ç”¢æ¥­æ–°èè‡ªå‹•çˆ¬å–")

# å´é‚Šæ¬„è¨­å®šåƒæ•¸
st.sidebar.header("è¨­å®šæœå°‹ç¯„åœ")
start_date_input = st.sidebar.date_input("é–‹å§‹æ—¥æœŸ", datetime(2024, 12, 26))
end_date_input = st.sidebar.date_input("çµæŸæ—¥æœŸ", datetime(2024, 12, 29))

# å°‡è¼¸å…¥è½‰ç‚º datetime æ ¼å¼ï¼ˆç›¸å®¹ä½ åŸæœ¬çš„é‚è¼¯ï¼‰
start_date = datetime.combine(start_date_input, datetime.min.time())
end_date = datetime.combine(end_date_input, datetime.max.time())

# --- 2. Google Sheets é€£ç·šè¨­å®š ---
# é€™è£¡æ”¹ç”¨ st.secrets è®€å–é‡‘é‘°ï¼Œä¸è¦æ”¾æª”æ¡ˆè·¯å¾‘
def init_gsheet():
    try:
        # åœ¨ Streamlit Cloud çš„ Secrets è¨­å®šä¸­è²¼ä¸Š JSON å…§å®¹
        service_account_info = json.loads(st.secrets["gcp_service_account"])
        gc = pygsheets.authorize(service_account_json=json.dumps(service_account_info))
        # è«‹ç¢ºä¿é€™æ¢ç¶²å€æ˜¯å°çš„ï¼Œæˆ–æ”¹æˆè®Šæ•¸
        spreadsheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA')
        return spreadsheet.worksheet_by_title('æœ€æ–°æ–°è')
    except Exception as e:
        st.error(f"Google Sheets é€£ç·šå¤±æ•—: {e}")
        return None

# --- 3. çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ (å°è£æˆ function) ---
def run_crawler(start_date, end_date):
    # (æ­¤è™•ä¿ç•™ä½ åŸæœ¬çš„ keywords, company_keywords, title_keywords æ¸…å–®)
    keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»", "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
    # ... (å…¶é¤˜é—œéµå­—æ¸…å–®çœç•¥ï¼Œè«‹ç…§èˆŠè²¼ä¸Š) ...

    dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []

    def append_news(title, url, date_obj, source, category):
        if start_date <= date_obj <= end_date:
            # ... (åŸæœ¬çš„éæ¿¾é‚è¼¯) ...
            pass # é€™è£¡è«‹è²¼ä¸Šä½ åŸæœ¬ append_news å…§çš„ç¨‹å¼ç¢¼

    progress_bar = st.progress(0)
    st.write("ğŸ” æ­£åœ¨æœå°‹å„å®¶åª’é«”...")

    # ... (é€™è£¡æ”¾ä½ åŸæœ¬çˆ¬ Yahoo, UDN, MoneyDJ, è‡ªç”±, ETtoday çš„è¿´åœˆ) ...
    # è¨˜å¾—åœ¨è¿´åœˆä¸­åŠ å…¥ st.write(f"æ­£åœ¨è™•ç†: {kw}") è®“ä½¿ç”¨è€…çŸ¥é“é€²åº¦

    # æœ€å¾Œå›å‚³ DataFrame
    final_df = pd.DataFrame({
        "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
        "æ¨™é¡Œé—œéµå­—": title_keyword_matches, "é—œè¯å…¬å¸": company_matches,
        "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": [""] * len(titles)
    })
    return final_df

# --- 4. ç¶²é åŸ·è¡ŒæŒ‰éˆ• ---
if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œçˆ¬èŸ²ä¸¦ä¸Šå‚³è‡³ Google Sheets"):
    sheet = init_gsheet()
    if sheet:
        with st.spinner('çˆ¬èŸ²åŸ·è¡Œä¸­ï¼Œè«‹ç¨å€™...'):
            df = run_crawler(start_date, end_date)
            
            if not df.empty:
                # å¯«å…¥ Google Sheet
                sheet.clear(start='A1')
                sheet.set_dataframe(df, 'A1')
                st.success(f"âœ… å®Œæˆï¼æˆåŠŸæŠ“å– {len(df)} ç­†è³‡æ–™ä¸¦å·²æ›´æ–°è‡³ Google Sheetsã€‚")
                st.dataframe(df) # ç¶²é é è¦½
            else:
                st.warning("æŸ¥ç„¡æ­¤æ—¥æœŸç¯„åœå…§çš„æ–°èã€‚")
import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.header import Header
from email.utils import formataddr
from urllib.parse import quote
import urllib.request as req
import bs4
from datetime import datetime
import datetime as dt
from pandas.tseries.offsets import BusinessDay
import warnings
import time as tt

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# --- 1. ä»‹é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

if 'edited_df' not in st.session_state:
    st.session_state.edited_df = pd.DataFrame()

# --- 2. å·¥å…·å‡½å¼ ---
def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except:
        return ""

def build_html_body(title_text, df, show_company_col=True):
    """
    å»ºç«‹ç¬¦åˆæ‚¨æ ¼å¼è¦æ±‚çš„ HTML è¡¨æ ¼
    show_company_col: æ§åˆ¶æ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€æ¬„ä½
    """
    intro = f"""
    {title_text}<br>
    <p style="color:gray; font-style:italic;">
    (æŠ“å–åŒ…å« <a href="#">ç‰¹å®šé—œéµå­—</a> çš„æ–°èï¼Œå¦‚æœéœ€è¦å¢åŠ æ–°èç¶²ç«™æˆ–é—œéµå­—è«‹è¯ç¹«JP)</p>
    """
    
    html_rows = ""
    for _, row in df.iterrows():
        # æ—¥æœŸæ ¼å¼åŒ–
        try:
            d_str = datetime.strptime(str(row['æ—¥æœŸ']), "%Y-%m-%d").strftime("%m/%d")
        except:
            d_str = str(row['æ—¥æœŸ'])

        # å…¬å¸é—œéµå­—é¡¯ç¤ºè™•ç†
        comp_kw = row.get('åŒ…å«å…¬å¸é—œéµå­—', '-')
        if pd.isna(comp_kw) or comp_kw == "": comp_kw = "-"

        # æ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦ç”¢ç”Ÿå…¬å¸æ¬„ä½çš„ HTML
        company_td = f"<td style='border:1px solid #333; padding:8px;'>{comp_kw}</td>" if show_company_col else ""

        html_rows += f"""
        <tr>
            <td style='border:1px solid #333; padding:8px;'>{d_str}</td>
            <td style='border:1px solid #333; padding:8px;'><a href='{row['ç¶²å€']}'>{row['æ¨™é¡Œ']}</a></td>
            {company_td}
            <td style='border:1px solid #333; padding:8px;'>{row.get('AI æ–°èæ‘˜è¦', '')}</td>
        </tr>"""
    
    # è¡¨é ­è™•ç†ï¼šæ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€è¡¨é ­
    company_th = '<th style="width:10%;">å…¬å¸</th>' if show_company_col else ''
    
    # èª¿æ•´æ‘˜è¦æ¬„ä½å¯¬åº¦ (å¦‚æœéš±è—å…¬å¸æ¬„ï¼Œæ‘˜è¦æ¬„å¯ä»¥å¯¬ä¸€é»)
    summary_width = "60%" if show_company_col else "70%"

    table_html = f"""
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 14px; border: 1px solid #333;">
        <thead><tr style="background-color: #f2f2f2; text-align: left;">
            <th style="width:5%;">æ—¥æœŸ</th>
            <th style="width:25%;">æ¨™é¡Œ</th>
            {company_th}
            <th style="width:{summary_width};">AIæ‘˜è¦</th>
        </tr></thead>
        <tbody>{html_rows}</tbody>
    </table>
    """
    return f"<html><body>{intro}{table_html}</body></html>"

def send_split_emails(df):
    sender = st.secrets["EMAIL_SENDER"]
    password = st.secrets["EMAIL_PASSWORD"]
    receiver = st.secrets["EMAIL_RECEIVER"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    # è¨­å®šé¡¯ç¤ºåç¨±
    SENDER_NAME = "æ¯æ—¥æ–°èå°å¹«æ‰‹" 
    RECEIVER_NAME = "éº—å‡èƒ½æºé›†åœ˜" 

    # é‚è¼¯ï¼šæœ‰å…¬å¸é—œéµå­— -> Group A (ç«¶æ¥­)ï¼›æ²’æœ‰ -> Group B (ç”¢æ¥­)
    def has_company_kw(val):
        if not val or pd.isna(val): return False
        s = str(val).strip().replace("-", "")
        return len(s) > 0

    group_a = df[df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]
    group_b = df[~df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, password)
            
            # ç™¼é€ Group A: ç«¶æ¥­æ–°è (é¡¯ç¤ºå…¬å¸æ¬„ä½)
            if not group_a.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç«¶æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=True -> é¡¯ç¤ºå…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç«¶æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_a, show_company_col=True), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç«¶æ¥­æ–°è ({len(group_a)} å°) å·²ç™¼é€")

            # ç™¼é€ Group B: ç”¢æ¥­æ–°è (éš±è—å…¬å¸æ¬„ä½)
            if not group_b.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç”¢æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=False -> éš±è—å…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç”¢æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_b, show_company_col=False), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç”¢æ¥­æ–°è ({len(group_b)} å°) å·²ç™¼é€")
        return True
    except Exception as e:
        st.error(f"ç™¼ä¿¡å¤±æ•—: {e}")
        return False

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èçˆ¬èŸ²")
    
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²", use_container_width=True):
        # å¼•å…¥ urllib3 ç”¨ä¾†é—œé–‰ SSL è­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        status_area = st.empty() 
        log_area = st.expander("ğŸ” çˆ¬èŸ²è©³ç´°æ—¥èªŒ (è‹¥æŠ“ä¸åˆ°è³‡æ–™è«‹é»é–‹æª¢æŸ¥)", expanded=True)
        
        with st.spinner("æ­£åœ¨å•Ÿå‹•å¼·åŠ›çˆ¬èŸ²..."):
            # æ™‚é–“è¨­å®š
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # åˆå§‹åŒ–å„²å­˜ç©ºé–“
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []
            
            # --- é—œéµå­—å®šç¾© (ç¢ºä¿æ”¾åœ¨æœ€å‰é¢) ---
            keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
            
            title_keywords = ["å°æ°´åŠ›","å…‰é›»","ç¶ èƒ½","ç¶ é›»","é¢¨èƒ½","å¤ªé™½èƒ½","å†ç”Ÿ","å„²èƒ½","æ¸›ç¢³","ESG","é›»æ± ","åœ°ç†±","é¢¨åŠ›","ç™¼é›»","é­šå¡­","åœŸåœ°","æ°´åŠ›","æ·¨é›¶","æ¼é›»","å…‰å„²","ä½åœ°åŠ›","å”®é›»","å°é›»","é…é›»","è¼¸é›»","å‡å£“","ç’°ç¤¾","ç”¨é›»å¤§æˆ¶","é¥‹ç·š","é›»è¡¨","è¡¨å‰","è¡¨å¾Œ","éœ€é‡åæ‡‰","é›»ç¶²","åœŸåœ°é–‹ç™¼","é›»å» ","å‚™è½‰","èª¿é »","PCS","EMS","BMS","é›»åŠ›äº¤æ˜“","ä½µç¶²","ç±Œè¨­","é¢¨é›»","é›»åƒ¹","é›»æ¥­","é¦™å¤¾è˜­","è¾²æ¥­è£œåŠ©","CPPA","è¾²é›»","è¾²æ¥­è¨­æ–½è¨±å¯","æ²¼æ°£","ç”Ÿè³ªèƒ½","Solar","PV","energy","solar","storage","å…‰ä¼","èƒ½æºæ”¿ç­–","ç¢³æ¬Š","ç¢³è²»","èº‰è³¼","èƒ½æºç½²","é›»æ¥­æ³•","èº‰è³¼è²»ç‡","æ¼é›»å…±ç”Ÿ"]

            company_keywords_raw = ["éº—å‡", "é™½å…‰ä¼ç‰¹å®¶é›»åŠ›" ,"é™½å…‰ä¼ç‰¹å®¶" ,"å°æ±½é›»ç¶ èƒ½" ,"å°æ±½é›»" ,"å¯Œå¨é›»åŠ›" ,"å¯Œå¨" ,"ç“¦ç‰¹å…ˆç”Ÿ" ,"ç“¦ç‰¹å…ˆç”Ÿ" ,"å—æ–¹é›»åŠ›" ,"" ,"èŠ±è“®ç¶ èƒ½" ,"" ,"çŸ³é–€å±±æ–°é›»åŠ›" ,"" ,"å¥‡ç•°æœæ–°èƒ½æº" ,"" ,"é¦–ç¾ç¶ èƒ½" ,"é¦–ç¾" ,"ä¸‰åœ°æ€ªç¸é›»åŠ›" ,"ä¸‰åœ°æ€ªç¸" ,"æ¨ºéŠ³ç¶ é›»ç§‘æŠ€" ,"æ¨ºéŠ³ç¶ é›»" ,"æ˜Ÿæ˜Ÿé›»åŠ›" ,"æ˜Ÿæ˜Ÿ" ,"å¤©èƒ½ç¶ é›»" ,"å¤©èƒ½ç¶ é›»" ,"é–‹é™½é›»åŠ›" ,"é–‹é™½" ,"åšæ›œé›»åŠ›" ,"åšæ›œ" ,"äºç¦å„²èƒ½" ,"äºç¦å„²èƒ½" ,"è«æ¯”ç¶ é›»" ,"è«æ¯”ç¶ é›»" ,"è¯åŸèƒ½æº" ,"è¯åŸ" ,"åç«£ç¶ èƒ½" ,"åç«£" ,"å¤§åŒæ™ºèƒ½" ,"å¤§åŒæ™ºèƒ½" ,"å¤ªé™½ç¥é›»åŠ›" ,"å¤ªé™½ç¥" ,"å¤§è‡ªç„¶èƒ½æºé›»æ¥­" ,"å¤§è‡ªç„¶èƒ½æºé›»æ¥­" ,"å¯¶å¯Œé›»åŠ›" ,"å¯¶å¯Œ" ,"ä¸­æ›œ" ,"ä¸­æ›œ" ,"é˜¿æ³¢ç¾…é›»åŠ›" ,"é˜¿æ³¢ç¾…" ,"ç“¦åŠ›é›»èƒ½" ,"ç“¦åŠ›é›»èƒ½" ,"é™½å…‰ç¶ é›»" ,"é™½å…‰ç¶ é›»" ,"çºŒèˆˆ" ,"çºŒèˆˆ" ,"èƒ½å…ƒè¶…å•†" ,"èƒ½å…ƒè¶…å•†" ,"å°ç£ç¢³è³‡ç”¢é›»æ¥­" ,"å°ç£ç¢³è³‡ç”¢é›»æ¥­" ,"åº·å±•é›»åŠ›" ,"åº·å±•" ,"å°åŒ–ç¶ èƒ½" ,"å°åŒ–" ,"ä¸Šæ™Ÿèƒ½æºç§‘æŠ€" ,"ä¸Šæ™Ÿèƒ½æº" ,"æ™¨æ˜Ÿé›»åŠ›" ,"æ™¨æ˜Ÿ" ,"å‚‘å‚…èƒ½æº" ,"å‚‘å‚…" ,"è©®å¯¦èƒ½æº" ,"è©®å¯¦" ,"å¯¶å³¶é™½å…‰é›»åŠ›äº‹æ¥­" ,"å¯¶å³¶é™½å…‰é›»åŠ›äº‹æ¥­" ,"èª æ–°é›»åŠ›" ,"" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"é›²è±¹èƒ½æº" ,"é¦™å°æ°¸çºŒ" ,"é¦™å°æ°¸çºŒ" ,"ç¾©é›»æ™ºæ…§èƒ½æº" ,"ç¾©é›»æ™ºæ…§" ,"å®‡è»’é›»æ¥­" ,"å®‡è»’é›»æ¥­" ,"ç–æš‰æ°¸çºŒé›»èƒ½" ,"ç–æš‰æ°¸çºŒé›»èƒ½" ,"æ›œè¶Šç¶ é›»" ,"æ›œè¶Šç¶ é›»" ,"è‰¾æ¶…çˆ¾é›»åŠ›" ,"è‰¾æ¶…çˆ¾" ,"èˆˆæ—ºèƒ½æº" ,"èˆˆæ—º" ,"èŒ‚æ¬£èƒ½æº" ,"èŒ‚æ¬£" ,"å’ŒåŒèƒ½æº" ,"å’ŒåŒ" ,"å®‰ç‘Ÿæ¨‚å¨" ,"å®‰ç‘Ÿæ¨‚å¨" ,"ä¸Šé›†èƒ½æº" ,"ä¸Šé›†" ,"å’Œæ½¤é›»åŠ›" ,"å’Œæ½¤" ,"æ¾æ¹–ç¶ é›»" ,"æ¾æ¹–ç¶ é›»" ,"ç¦¾ä¸°é›»åŠ›" ,"ç¦¾ä¸°" ,"æ–°é‘«é›»åŠ›" ,"æ–°é‘«" ,"å°é”èƒ½æº" ,"å°é”" ,"ç²¾è¯èƒ½æº" ,"ç²¾è¯" ,"åœ‹ç¢©èƒ½æº" ,"åœ‹ç¢©" ,"æ°¸é¤˜æ™ºèƒ½" ,"æ°¸é¤˜æ™ºèƒ½" ,"æ†åˆ©é›»èƒ½" ,"æ†åˆ©é›»èƒ½" ,"è‰¾åœ°é›»åŠ›" ,"è‰¾åœ°" ,"æ–°æ™¶å¤ªé™½å…‰é›»ç§‘æŠ€" ,"æ–°æ™¶å¤ªé™½å…‰é›»" ,"å¤©å‹¢èƒ½æº" ,"å¤©å‹¢" ,"æ‰¿ç ”èƒ½æºç§‘æŠ€" ,"æ‰¿ç ”èƒ½æº" ,"çµ±ç›Šèƒ½æº" ,"çµ±ç›Š" ,"æ€¡å’Œç¶ é›»è¶…å•†" ,"æ€¡å’Œç¶ é›»è¶…å•†" ,"ä¸­è¯ç³»çµ±æ•´åˆ" ,"ä¸­è¯ç³»çµ±æ•´åˆ" ,"è£•é´»èƒ½æº" ,"è£•é´»" ,"æ˜å¾½é›»åŠ›" ,"æ˜å¾½" ,"å¼˜æ˜Œæ³°" ,"å¼˜æ˜Œæ³°" ,"æ˜¶å³°ç¶ èƒ½ç§‘æŠ€" ,"æ˜¶å³°ç¶ èƒ½" ,"æˆç¶ èƒ½" ,"æœ‰æˆ" ,"åè¬ä¼ç‰¹é›»åŠ›" ,"åè¬ä¼ç‰¹" ,"å‹é”é›»åŠ›" ,"å‹é”" ,"æ¾¤ç”Ÿèƒ½æº" ,"æ¾¤ç”Ÿ" ,"å…‰åˆä½œç”¨" ,"å…‰åˆä½œç”¨" ,"æ˜•æ˜é›»åŠ›" ,"æ˜•æ˜" ,"é´»æ™¶æ–°ç§‘æŠ€" ,"é´»æ™¶æ–°" ,"æ¯“ç›ˆ" ,"æ¯“ç›ˆ" ,"å¤©éº‹é›»åŠ›" ,"å¤©éº‹" ,"æ–°å…‰æºé›»åŠ›" ,"æ–°å…‰æº" ,"æ†ç«‹èƒ½æº" ,"æ†ç«‹" ,"æ˜Ÿè¾°é›»åŠ›" ,"æ˜Ÿè¾°" ,"è¾°æ˜‡èƒ½æº" ,"è¾°æ˜‡" ,"åº·èª èƒ½æº" ,"åº·èª " ,"å¯¬åŸŸèƒ½æº" ,"å¯¬åŸŸ" ,"å¤§å‰µé›»åŠ›" ,"å¤§å‰µ" ,"å¤ªå‰µèƒ½æº" ,"å¤ªå‰µ" ,"å¤§çŒ©çŒ©é›»èƒ½äº¤æ˜“" ,"å¤§çŒ©çŒ©é›»èƒ½äº¤æ˜“" ,"å¥‰å¤©é›»åŠ›" ,"å¥‰å¤©" ,"å°ç£å¨è¿ªå…‹è‰¾å…§æ–¯é”èƒ½æº" ,"å°ç£å¨è¿ªå…‹è‰¾å…§æ–¯é”" ,"è‚²æˆé›»åŠ›" ,"" ,"æ©™é‘«é›»åŠ›" ,"æ©™é‘«" ,"è€€é¼è³‡æºå¾ªç’°" ,"è€€é¼è³‡æºå¾ªç’°" ,"ä¸­æ—¥é›»åŠ›" ,"" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"å°ç£æ™ºèƒ½æ¼é›»ç§‘æŠ€" ,"å°ç£æ™ºèƒ½æ¼é›»" ,"æµ·åˆ©æ™®æ–°èƒ½æº" ,"æµ·åˆ©æ™®" ,"ç‰¹èˆˆèƒ½æºé¡§å•" ,"ç‰¹èˆˆèƒ½æºé¡§å•" ,"å°ç£æ™ºæ…§é›»èƒ½" ,"å°ç£æ™ºæ…§é›»èƒ½" ,"è¯æ—­èƒ½æºé–‹ç™¼" ,"è¯æ—­èƒ½æºé–‹ç™¼" ,"éŒ¦æŒ¯èƒ½æº" ,"éŒ¦æŒ¯" ,"å®‰èƒ½é›»æ¥­" ,"å®‰èƒ½é›»æ¥­" ,"é‡‘è±¬èƒ½æºç§‘æŠ€" ,"é‡‘è±¬èƒ½æº" ,"å°å¡‘ç¶ é›»" ,"å°å¡‘ç¶ é›»" ,"è¯ç’½èƒ½æº" ,"è¯ç’½" ,"è‚²æ¸²æŠ•è³‡" ,"è‚²æ¸²æŠ•è³‡æœ‰é™å…¬å¸" ,"æ­æ‚…èƒ½æº" ,"æ­æ‚…" ,"åº­æ—" ,"åº­æ—" ,"æ™Ÿé‹ç§‘æŠ€" ,"æ™Ÿé‹ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ˜Ÿå´´é›»åŠ›" ,"æ˜Ÿå´´" ,"æ¼¢ç‚ºç§‘æŠ€å·¥ç¨‹" ,"æ¼¢ç‚ºç§‘æŠ€å·¥ç¨‹æœ‰é™å…¬å¸" ,"ç«‹è±å…‰èƒ½" ,"ç«‹è±å…‰èƒ½" ,"ç‰ç’ƒå…‰ç¶ èƒ½" ,"ç‰ç’ƒå…‰" ,"é“é”çˆ¾èƒ½æº" ,"" ,"æ±æ³°ç¶ èƒ½æŠ•è³‡" ,"æ±æ³°ç¶ èƒ½æŠ•è³‡æœ‰é™å…¬å¸" ,"å¯Œé™½èƒ½é–‹ç™¼" ,"å¯Œé™½èƒ½é–‹ç™¼" ,"å‰ç¥¥ç§‘æŠ€" ,"å‰ç¥¥" ,"å‡±æ™ºç¶ èƒ½ç§‘æŠ€" ,"å‡±æ™ºç¶ èƒ½" ,"æ°¸è±å¤ªé™½èƒ½èƒ½æº" ,"æ°¸è±å¤ªé™½èƒ½èƒ½æºæœ‰é™å…¬å¸" ,"è·¯åŠ å¤ªé™½èƒ½æŠ•è³‡é¡§å•" ,"è·¯åŠ å¤ªé™½èƒ½æŠ•è³‡é¡§å•" ,"å¦‚æ™…ç¶ èƒ½é–‹ç™¼" ,"å¦‚æ™…ç¶ èƒ½é–‹ç™¼æœ‰é™å…¬å¸" ,"åŠ›å±±ç¶ èƒ½ç§‘æŠ€" ,"åŠ›å±±ç¶ èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ±ä¹‹å„„ç¶ èƒ½" ,"æ±ä¹‹å„„ç¶ èƒ½æœ‰é™å…¬å¸" ,"è¯å®èšèƒ½ç§‘æŠ€" ,"è¯å®èšèƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å¤ªèƒ½ç³»çµ±" ,"å¤ªèƒ½ç³»çµ±" ,"æ˜“æ™¶ç¶ èƒ½ç³»çµ±" ,"æ˜“æ™¶ç¶ èƒ½ç³»çµ±æœ‰é™å…¬å¸" ,"æ°¸æ»”ç¶ èƒ½" ,"æ°¸æ»”" ,"å°ç£æ‰€æ¨‚å¤ªé™½èƒ½ç§‘æŠ€" ,"å°ç£æ‰€æ¨‚å¤ªé™½èƒ½" ,"ç¿°å¯èƒ½æº" ,"ç¿°å¯" ,"å’Œåˆè³‡æºç¶ èƒ½" ,"å’Œåˆè³‡æºç¶ èƒ½æœ‰é™å…¬å¸" ,"ç¶­çŸ¥ç§‘æŠ€" ,"ç¶­çŸ¥ã€è´ŠåŠ©ã€‘" ,"åŠ é›²è¯ç¶²" ,"åŠ é›²è¯ç¶²" ,"æ±æ­¦é›»æ©Ÿå·¥æ¥­" ,"æ±æ­¦é›»æ©Ÿå·¥æ¥­" ,"å‰é€²ç¶ èƒ½ç§‘æŠ€" ,"å‰é€²ç¶ èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å…‰æ—­ç›ˆç§‘æŠ€" ,"å…‰æ—­ç›ˆ" ,"æ™´æ£ å¯¬èƒ½æºå·¥ç¨‹" ,"æ™´æ£ å¯¬èƒ½æºå·¥ç¨‹æœ‰é™å…¬å¸" ,"å‡±ç±³å…‹å¯¦æ¥­" ,"å‡±ç±³å…‹å¯¦æ¥­" ,"å¤§æ—¥é ­" ,"å¤§æ—¥é ­" ,"æ–°æ™¶å…‰é›»" ,"æ–°æ™¶å…‰é›»" ,"æ†åˆ©èƒ½æº" ,"æ†åˆ©" ,"å…‰é¼èƒ½æºç§‘æŠ€" ,"å…‰é¼èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸" ,"ç’°äºå…‰é›»" ,"ç’°äºå…‰é›»" ,"å®£å† " ,"å®£å† " ,"è¡†å´´èƒ½æº" ,"è¡†å´´" ,"æ¨‚é™½èƒ½æº" ,"æ¨‚é™½èƒ½æºæœ‰é™å…¬å¸" ,"å°ç£å’Œæš„ç¶ èƒ½" ,"å°ç£å’Œæš„" ,"è–å±•å…‰èƒ½" ,"è–å±•å…‰èƒ½" ,"å‰µç¿èƒ½æº" ,"å‰µç¿" ,"ç™¾åˆ©å¯Œèƒ½æº" ,"ç™¾åˆ©å¯Œ" ,"é‡‘é›»ç™¼èƒ½æº" ,"é‡‘é›»ç™¼èƒ½æºæœ‰é™å…¬å¸" ,"é¼æ‰¿èƒ½æºç§‘æŠ€" ,"é¼æ‰¿èƒ½æº" ,"æ˜¶è€€é–‹ç™¼" ,"æ˜¶è€€é–‹ç™¼æœ‰é™å…¬å¸" ,"æ˜Ÿèƒ½" ,"æ˜Ÿèƒ½" ,"æ—¥å‹å†ç”Ÿèƒ½æº" ,"æ—¥å‹å†ç”Ÿèƒ½æºæœ‰é™å…¬å¸(å°ç£å¤§æ ¹å…¬å¸é›†åœ˜)" ,"åœ‹è»’ç§‘æŠ€" ,"åœ‹è»’" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"é›²è±¹èƒ½æº" ,"æ˜‡éˆºå…‰é›»" ,"æ˜‡éˆºå…‰é›»" ,"ç¶ é †ç§‘æŠ€" ,"ç¶ é †" ,"è£•é›»èƒ½æº" ,"è£•é›»" ,"æš˜å…‰ç¶ èƒ½å¯¦æ¥­" ,"æš˜å…‰ç¶ èƒ½å¯¦æ¥­" ,"å‡¡å±•ç¶ èƒ½ç§‘æŠ€" ,"å‡¡å±•ç¶ èƒ½" ,"æ—­èª ç¶ èƒ½" ,"æ—­èª ç¶ èƒ½æœ‰é™å…¬å¸" ,"å¤§ç€šé‹¼éµ" ,"å¤§ç€šé‹¼éµ" ,"ç¶ è‘³èƒ½æºç§‘æŠ€" ,"ç¶ è‘³èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸" ,"ä¸­ç§Ÿé›»åŠ›ç§‘æŠ€" ,"ä¸­ç§Ÿé›»åŠ›" ,"æ­å¾—èƒ½æºå·¥ç¨‹" ,"æ­å¾—èƒ½æºå·¥ç¨‹æœ‰é™å…¬å¸" ,"å…‰ç…œèƒ½æº" ,"å…‰ç…œ" ,"æœæ—¥èƒ½æº" ,"æœæ—¥èƒ½æºæœ‰é™å…¬å¸" ,"å˜‰æ¯…é”å…‰é›»ä¼æ¥­" ,"å˜‰æ¯…é”å…‰é›»ä¼æ¥­" ,"å§‹å¾©èƒ½æº" ,"å§‹å¾©" ,"éŠ˜æ‡‹å·¥æ¥­" ,"éŠ˜æ‡‹å·¥æ¥­" ,"å®‡è»’é‹¼éµå·¥ç¨‹" ,"" ,"æ™¶æˆèƒ½æº" ,"æ™¶æˆ" ,"å…ƒæ™¶å¤ªé™½èƒ½ç§‘æŠ€" ,"å…ƒæ™¶å¤ªé™½èƒ½" ,"å…†ä¿¡é›»é€šç§‘æŠ€" ,"å…†ä¿¡é›»é€šç§‘æŠ€æœ‰é™å…¬å¸" ,"ç™¾ç››èƒ½æºç§‘æŠ€" ,"ç™¾ç››èƒ½æº" ,"ç¦¾åŸæ–°èƒ½æºç§‘æŠ€" ,"ç¦¾åŸæ–°èƒ½æº" ,"æ—­å¤©èƒ½æº" ,"æ—­å¤©" ,"å…¨æ—¥å…‰" ,"å…¨æ—¥å…‰æœ‰é™å…¬å¸" ,"é¨°æšç¶ é›»" ,"é¨°æšç¶ é›»æœ‰é™å…¬å¸" ,"ç¶ è¾²é›»ç§‘" ,"ç¶ è¾²é›»ç§‘" ,"è‡ºé¹½ç¶ èƒ½" ,"è‡ºé¹½" ,"æ˜•æ¯…ç§‘æŠ€" ,"æ˜•æ¯…ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ½”åŠ›èƒ½æºäº‹æ¥­" ,"æ½”åŠ›èƒ½æºäº‹æ¥­æœ‰é™å…¬å¸" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"é¦–ç¾èƒ½æº" ,"é¦–ç¾" ,"æ°¸æ—¥æ˜‡ç¶ èƒ½" ,"æ°¸æ—¥æ˜‡ç¶ èƒ½æœ‰é™å…¬å¸" ,"å¤çˆ¾ç‰¹æ‹‰å¤ªé™½èƒ½ç§‘æŠ€" ,"å¤çˆ¾ç‰¹æ‹‰å¤ªé™½èƒ½" ,"ç’°çƒå¤§å®‡å®™å¤ªé™½èƒ½å·¥æ¥­" ,"ç’°çƒå¤§å®‡å®™å¤ªé™½èƒ½å·¥æ¥­æœ‰é™å…¬å¸" ,"å‡Œç©æ‡‰ç”¨ç§‘æŠ€" ,"å‡Œç©æ‡‰ç”¨" ,"å´‘é¼ç¶ èƒ½ç’°ä¿" ,"å´‘é¼ç¶ èƒ½ç’°ä¿" ,"ç››é½Šç¶ èƒ½" ,"ç››é½Š" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å—äºå…‰é›»" ,"å—äºå…‰é›»" ,"å®¶ç´³èƒ½æº" ,"å®¶ç´³" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½æœ‰é™å…¬å¸" ,"å£«èƒ½ç§‘æŠ€" ,"å£«èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"é—œéµæ‡‰ç”¨ç§‘æŠ€" ,"é—œéµæ‡‰ç”¨" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"å‘é™½å„ªèƒ½é›»åŠ›" ,"å‘é™½å„ªèƒ½" ,"ä¿¡é‚¦é›»å­" ,"ä¿¡é‚¦é›»å­" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å°ç£é”äº¨èƒ½æºç§‘æŠ€" ,"å°ç£é”äº¨èƒ½æº" ,"å¤©æ³°èƒ½æº" ,"å¤©æ³°" ,"æ³“ç­Œç§‘æŠ€" ,"æ³“ç­Œ" ,"æˆç²¾å¯†" ,"æœ‰æˆç²¾å¯†" ,"æ›œæ˜‡ç¶ èƒ½" ,"æ›œæ˜‡" ,"é‡‘é™½æ©Ÿé›»å·¥ç¨‹" ,"é‡‘é™½æ©Ÿé›»å·¥ç¨‹æœ‰é™å…¬å¸" ,"æ±å…ƒé›»æ©Ÿ" ,"æ±å…ƒé›»æ©Ÿ" ,"å…†æ´‹å¤ªé™½èƒ½æº" ,"å…†æ´‹å¤ªé™½èƒ½æºæœ‰é™å…¬å¸" ,"é‘«ç›ˆèƒ½æº" ,"é‘«ç›ˆ" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"æ˜è»’ç§‘æŠ€" ,"æ˜è»’ç§‘æŠ€æœ‰é™å…¬å¸" ,"ç´¹æ´²èˆˆæ¥­" ,"ç´¹æ´²èˆˆæ¥­" ,"åšç››å…‰é›»ç§‘æŠ€" ,"åšç››å…‰é›»ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ³“å¾·èƒ½æºç§‘æŠ€" ,"æ³“å¾·èƒ½æº" ,"ç¶ æºç§‘æŠ€" ,"ç¶ æº" ,"æ—¥å±±èƒ½æºç§‘æŠ€" ,"æ—¥å±±èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸"]
            company_keywords = [k.strip() for k in company_keywords_raw if k.strip() != ""]

            # --- è¼”åŠ©å‡½å¼ ---
            def parse_flexible_date(date_text):
                if not date_text: return None
                clean_text = date_text.replace("(", "").replace(")", "").strip().split(" ")[0]
                formats = ["%Y/%m/%d", "%Y-%m-%d", "%Y.%m.%d", "%Y%m%d"]
                for fmt in formats:
                    try: return datetime.strptime(clean_text, fmt)
                    except ValueError: continue
                return None

            def find_company_keywords(text):
                return [k for k in company_keywords if k in text]

            # çµ±è¨ˆæ•¸æ“š
            stats = {"Yahoo": 0, "UDN": 0, "MoneyDJ": 0, "LTN": 0, "ETtoday": 0}

            # ==========================================
            # 1. Yahoo çˆ¬èŸ²
            # ==========================================
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
            for kw in keywords:
                try:
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={quote(kw)}", headers=headers, timeout=5)
                    soup = BeautifulSoup(res.text, "html.parser")
                    articles = soup.select("li div[class*='Cf']")
                    
                    for art in articles:
                        try:
                            a_tag = art.find("a")
                            if not a_tag: continue
                            title = a_tag.text.strip()
                            href = a_tag["href"]
                            full_link = href if href.startswith("http") else f"https://tw.news.yahoo.com{href}"
                            
                            date_obj = None
                            meta_div = art.find("div", class_="C(#959595)")
                            if meta_div:
                                time_str = meta_div.text.strip().split("â€¢")[-1].strip()
                                today = datetime.now()
                                if "å¤©å‰" in time_str:
                                    date_obj = today - dt.timedelta(days=int(time_str.replace("å¤©å‰", "")))
                                elif "å°æ™‚" in time_str or "åˆ†é˜" in time_str:
                                    date_obj = today
                                elif "å¹´" in time_str:
                                    d_s = time_str.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0]
                                    date_obj = parse_flexible_date(d_s)
                            
                            if date_obj and start_date_obj <= date_obj <= end_date_obj:
                                if any(k in title for k in title_keywords):
                                    dates.append(date_obj.strftime("%Y-%m-%d"))
                                    sources.append("Yahoo")
                                    categories.append(kw)
                                    titles.append(title)
                                    links.append(full_link)
                                    stats["Yahoo"] += 1
                                    
                                    mk = [k for k in title_keywords if k in title]
                                    mck = find_company_keywords(title)
                                    title_keyword_matches.append(",".join(mk))
                                    company_matches.append(",".join(mck) if mck else "-")
                        except: continue
                except: continue
            
            log_area.write(f"Yahoo æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['Yahoo']} ç­†")

            # ==========================================
            # 2. è‡ªç”±æ™‚å ± (LTN)
            # ==========================================
            ltn_urls = [
                ("https://news.ltn.com.tw/topic/å†ç”Ÿèƒ½æº", "å†ç”Ÿèƒ½æº"),
                ("https://news.ltn.com.tw/topic/å¤ªé™½èƒ½", "å¤ªé™½èƒ½"),
                ("https://news.ltn.com.tw/topic/é¢¨åŠ›ç™¼é›»", "é¢¨é›»"),
                ("https://news.ltn.com.tw/topic/ç¶ é›»", "ç¶ é›»"),
            ]
            
            for url, cat in ltn_urls:
                try:
                    res = requests.get(url, headers=headers, timeout=10)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    items = soup.select("ul.searchlist li") or \
                            soup.select("div.whitecon li") or \
                            soup.select("ul.list li") or \
                            soup.select("div.boxTitle li")
                    
                    if not items:
                        log_area.warning(f"LTN: åœ¨ {cat} æ‰¾ä¸åˆ°ä»»ä½• li å…ƒç´ ")

                    for item in items:
                        if "class" in item.attrs and "ad" in item.attrs["class"]: continue

                        a_tag = item.find("a")
                        if not a_tag: continue
                        
                        href = a_tag.get("href", "")
                        title = a_tag.get("title") or a_tag.text.strip()
                        
                        if not title or not href: continue
                        if not href.startswith("http"):
                            href = "https://news.ltn.com.tw/" + href.lstrip("/")
                        
                        date_obj = None
                        time_tag = item.find("span", class_="time")
                        if time_tag:
                            date_obj = parse_flexible_date(time_tag.text)
                        
                        if date_obj:
                            if start_date_obj <= date_obj <= end_date_obj:
                                matched_kws = [k for k in title_keywords if k in title]
                                if matched_kws:
                                    dates.append(date_obj.strftime("%Y-%m-%d"))
                                    sources.append("è‡ªç”±æ™‚å ±")
                                    categories.append(cat)
                                    titles.append(title)
                                    links.append(href)
                                    title_keyword_matches.append(",".join(matched_kws))
                                    mck = find_company_keywords(title)
                                    company_matches.append(",".join(mck) if mck else "-")
                                    stats["LTN"] += 1
                except Exception as e:
                    log_area.error(f"LTN Error ({cat}): {e}")

            log_area.write(f"è‡ªç”±æ™‚å ± æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['LTN']} ç­†")

            # ==========================================
            # 3. ETtoday (ä¿®æ­£ SSL å•é¡Œ)
            # ==========================================
            for kw in keywords:
                try:
                    u = f"https://www.ettoday.net/news_search/doSearch.php?keywords={quote(kw)}&idx=1"
                    # é—œéµä¿®æ­£ï¼šåŠ å…¥ verify=False å¿½ç•¥ SSL é©—è­‰
                    res = requests.get(u, headers=headers, timeout=10, verify=False)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    items = soup.select("div.archive_list div.box_2")
                    if not items: items = soup.select("div.result_archive div.box_2")

                    for art in items:
                        h2 = art.find("h2")
                        if not h2 or not h2.find("a"): continue
                        
                        title = h2.find("a").text.strip()
                        href = h2.find("a")["href"]
                        
                        date_obj = None
                        date_tag = art.find("span", class_="date")
                        if date_tag:
                            d_text = date_tag.text.strip().split(")")[0].replace("(", "")
                            date_obj = parse_flexible_date(d_text)
                        
                        if date_obj and start_date_obj <= date_obj <= end_date_obj:
                             if any(k in title for k in title_keywords):
                                dates.append(date_obj.strftime("%Y-%m-%d"))
                                sources.append("ETtoday")
                                categories.append(kw)
                                titles.append(title)
                                links.append(href)
                                stats["ETtoday"] += 1
                                
                                mk = [k for k in title_keywords if k in title]
                                mck = find_company_keywords(title)
                                title_keyword_matches.append(",".join(mk))
                                company_matches.append(",".join(mck) if mck else "-")
                except Exception as e:
                    log_area.error(f"ETtoday Error ({kw}): {e}")

            log_area.write(f"ETtoday æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['ETtoday']} ç­†")

             # --- UDN çˆ¬èŸ² ---
            for i in range(len(keywords)):
                try:
                    kw = keywords[i]
                    url = f"https://udn.com/search/word/2/{quote(kw)}"
                    req_obj = req.Request(url, headers={"User-Agent": "Mozilla/5.0"})
                    with req.urlopen(req_obj) as response:
                        data = response.read().decode("utf-8")
                    soup = bs4.BeautifulSoup(data, "html.parser")
                    ti_box = soup.find("div", class_="context-box__content story-list__holder story-list__holder--full")
                    if not ti_box: continue
                    ti_h2 = ti_box.find_all("h2")
                    ti_time = ti_box.find_all("time", class_="story-list__time")
                    for l, title_tag in enumerate(ti_h2):
                        a_tag = title_tag.find("a")
                        if not a_tag or l >= len(ti_time): continue
                        title = a_tag.get_text(strip=True)
                        href = a_tag.get("href")
                        try:
                            date_obj = datetime.strptime(ti_time[l].get_text(strip=True)[:10], "%Y-%m-%d")
                            append_news(title, href, date_obj, "UDN", kw)
                        except: continue

            # --- å½™æ•´çµæœ ---
            if titles:
                df = pd.DataFrame({
                    "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                    "åŒ…å«æ¨™é¡Œé—œéµå­—": title_keyword_matches, "åŒ…å«å…¬å¸é—œéµå­—": company_matches,
                    "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": ""
                }).drop_duplicates(subset=["æ¨™é¡Œ"]).sort_values(by="æ—¥æœŸ", ascending=False).reset_index(drop=True)
                
                df["åŸæ–‡é€£çµ"] = df["ç¶²å€"] 
                st.session_state.edited_df = df
                st.success(f"âœ… æŠ“å–å®Œæˆï¼æœ¬æ¬¡å…±æŠ“åˆ° {len(df)} ç­†ã€‚ (Yahoo:{stats['Yahoo']}, LTN:{stats['LTN']}, ETtoday:{stats['ETtoday']})")
            else:
                st.error("âŒ ä¾ç„¶æŸ¥ç„¡æ–°èã€‚è«‹å±•é–‹ä¸Šæ–¹çš„ã€Œè©³ç´°æ—¥èªŒã€æª¢æŸ¥ã€‚")
                st.info(f"åµæ¸¬ç¯„åœ: {s_date} åˆ° {e_date}")

    # æ­¥é©ŸäºŒ
    st.header("2ï¸âƒ£ ç”¢ç”ŸAIæ‘˜è¦")
    if st.button("é»æˆ‘", use_container_width=True):
        if not st.session_state.edited_df.empty:
            client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
            for idx, row in st.session_state.edited_df.iterrows():
                if not row['AI æ–°èæ‘˜è¦']:
                    st.write(f"æ‘˜è¦ç”¢ç”Ÿä¸­: {row['æ¨™é¡Œ'][:15]}...")
                    text = extract_webpage_text(row['ç¶²å€'])
                    if text:
                        try:
                            res = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å€‹å­—ï¼š\n\n{text[:2500]}"}]
                            )
                            st.session_state.edited_df.at[idx, 'AI æ–°èæ‘˜è¦'] = res.choices[0].message.content.strip()
                        except: pass
            st.rerun()

    st.divider()

    # æ­¥é©Ÿä¸‰
    st.header("3ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ç™¼ä¿¡çµ¦å…¨å…¬å¸", use_container_width=True):
        if not st.session_state.edited_df.empty:
            if send_split_emails(st.session_state.edited_df):
                st.balloons()
                st.success("âœ… æ‰€æœ‰ä¿¡ä»¶ç™¼é€å®Œæˆï¼")
        else:
            st.warning("âš ï¸ ç•«é¢ä¸Šæ²’æœ‰è³‡æ–™ã€‚")

# --- 4. ä¸»ç•«é¢ ---
st.write("### ğŸ“ ç·¨è¼¯ç™¼ä½ˆæ¸…å–®")
st.caption("æç¤ºï¼šé¸å–è¡Œä¸¦æŒ‰ Delete å¯åˆªé™¤ï¼›æ¬„ä½å¯ä¾æ“šç™¼ä¿¡éœ€æ±‚æ‰‹å‹•ä¿®æ”¹ï¼Œæœ‰å…¬å¸é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç«¶æ¥­æ–°èã€ã€æ²’é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç”¢æ¥­æ–°èã€ã€‚")

if not st.session_state.edited_df.empty:
    st.session_state.edited_df = st.data_editor(
        st.session_state.edited_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "æ—¥æœŸ": st.column_config.TextColumn("æ—¥æœŸ", disabled=True),
            "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="large"),
            "åŸæ–‡é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="(æŸ¥çœ‹)", width="small"),
            "ç¶²å€": None, # éš±è—åŸå§‹ç¶²å€
            "åŒ…å«å…¬å¸é—œéµå­—": st.column_config.TextColumn("å…¬å¸é—œéµå­—", width="medium"),
            "AI æ–°èæ‘˜è¦": st.column_config.TextColumn("AI æ–°èæ‘˜è¦", width="large")
        },
        column_order=["æ—¥æœŸ", "ä¾†æº", "æ¨™é¡Œ", "åŸæ–‡é€£çµ", "åŒ…å«å…¬å¸é—œéµå­—", "AI æ–°èæ‘˜è¦"]
    )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆå¾å·¦å´åŸ·è¡Œã€Œæ­¥é©Ÿä¸€ã€æŠ“å–æ–°èã€‚")

import streamlit as st
import pandas as pd
import datetime as dt
from datetime import datetime
import pygsheets
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import time as tt
import json
import gspread
from google.oauth2.service_account import Credentials
from openai import OpenAI
from pandas.tseries.offsets import BusinessDay

# --- 1. åŸºç¤è¨­å®šèˆ‡é—œéµå­— ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç³»çµ±", page_icon="âš¡", layout="wide")

KEYWORDS = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»", "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
COMPANY_KEYWORDS = ["éº—å‡", "é›²è±¹èƒ½æº", "æ³“å¾·èƒ½æº", "æ£®å´´èƒ½æº", "é€²é‡‘ç”Ÿ", "é–‹é™½é›»åŠ›", "æ˜Ÿæ˜Ÿé›»åŠ›", "ä¸­ç§Ÿé›»åŠ›", "å…ƒæ™¶", "å‹é”é›»åŠ›"]
TITLE_KEYWORDS = ["å…‰é›»", "ç¶ èƒ½", "ç¶ é›»", "å¤ªé™½èƒ½", "å†ç”Ÿ", "å„²èƒ½", "æ¸›ç¢³", "ESG", "ç™¼é›»", "æ¼é›»", "å…‰å„²", "é›»åƒ¹"]

# --- 2. å·¥å…·å‡½å¼ ---
def get_pygsheets_wks():
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    gc = pygsheets.authorize(service_account_json=json.dumps(service_account_info))
    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA')
    return sh.worksheet_by_title('æœ€æ–°æ–°è')

def get_gspread_wks():
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    creds = Credentials.from_service_account_info(service_account_info, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    gc = gspread.authorize(creds)
    return gc.open_by_key("1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA").worksheet("æœ€æ–°æ–°è")

def extract_webpage_text(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except: return ""

# --- 3. å´é‚Šæ¬„å·¥ä½œæµ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±")
    
    # æ­¥é©Ÿä¸€ï¼šçˆ¬èŸ²
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²ä¸¦ä¸Šå‚³", use_container_width=True):
        with st.spinner("æ­£åœ¨çˆ¬å–åª’é«”è³‡æ–™..."):
            start_dt = datetime.combine(s_date, datetime.min.time())
            end_dt = datetime.combine(e_date, datetime.max.time())
            
            data_list = []
            
            # --- çˆ¬èŸ²å¯¦ä½œ (Yahoo) ---
            for kw in KEYWORDS:
                url = f"https://tw.news.yahoo.com/search?p={quote(kw)}"
                try:
                    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
                    soup = BeautifulSoup(res.text, "html.parser")
                    for art in soup.select("li div[class*='Cf']"):
                        a = art.find("a")
                        m = art.find("div", class_="C(#959595)")
                        if not a or not m: continue
                        
                        title = a.text.strip()
                        link = a["href"] if a["href"].startswith("http") else f"https://tw.news.yahoo.com{a['href']}"
                        
                        # æ—¥æœŸè™•ç†
                        t_str = m.text.strip().split("â€¢")[-1].strip()
                        d_obj = datetime.now()
                        if "å¤©å‰" in t_str: d_obj -= dt.timedelta(days=int(t_str.replace("å¤©å‰","")))
                        elif "å°æ™‚" in t_str or "åˆ†é˜" in t_str: pass
                        else:
                            try:
                                clean_d = t_str.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").strip()
                                d_obj = datetime.strptime(clean_d.split()[0], "%Y-%m-%d")
                            except: continue
                        
                        if start_dt <= d_obj <= end_dt:
                            m_title = [k for k in TITLE_KEYWORDS if k in title]
                            if m_title:
                                m_comp = [k for k in COMPANY_KEYWORDS if k in title]
                                data_list.append([d_obj.strftime("%Y-%m-%d"), "Yahoo", kw, title, link, ", ".join(m_title), ", ".join(m_comp)])
                except: continue

            # --- è½‰ç‚º DataFrame ä¸¦å¯«å…¥ ---
            if data_list:
                df = pd.DataFrame(data_list, columns=["æ—¥æœŸ", "ä¾†æº", "åˆ†é¡", "æ¨™é¡Œ", "æ–°èç¶²å€", "åŒ…å«æ¨™é¡Œé—œéµå­—", "åŒ…å«å…¬å¸é—œéµå­—"])
                df["AI æ–°èæ‘˜è¦"] = ""
                df = df.drop_duplicates(subset=["æ¨™é¡Œ"])
                
                wks = get_pygsheets_wks()
                wks.clear(start='A1')
                wks.set_dataframe(df, 'A1')
                st.success(f"æ­¥é©Ÿä¸€å®Œæˆï¼æŠ“å–åˆ° {len(df)} ç­†ã€‚")
            else:
                st.error("æ‰¾ä¸åˆ°ç¬¦åˆæ—¥æœŸèˆ‡é—œéµå­—çš„æ–°èã€‚")

    st.divider()
    st.header("2ï¸âƒ£ äººå·¥å¯©æ ¸æ–‡ç« ")
    st.link_button("ğŸ“‚ é–‹å•Ÿ Sheets åˆªæ¸›", "https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA/edit", use_container_width=True)

    st.divider()
    st.header("3ï¸âƒ£ AI è‡ªå‹•æ‘˜è¦")
    if st.button("ğŸ¤– åŸ·è¡Œ OpenAI æ‘˜è¦", use_container_width=True):
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
        sheet = get_gspread_wks()
        rows = sheet.get_all_values()
        for idx, row in enumerate(rows[1:], start=2):
            url, summary = row[4], row[7] if len(row) > 7 else ""
            if url.strip() and not summary.strip():
                st.write(f"æ‘˜è¦ä¸­: {url[:30]}...")
                text = extract_webpage_text(url)
                if text:
                    res = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å­—ï¼š\n\n{text[:2000]}"}]
                    )
                    sheet.update_cell(idx, 8, res.choices[0].message.content.strip())
        st.success("æ­¥é©Ÿä¸‰å®Œæˆï¼")

    st.divider()
    st.header("4ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ğŸ“§ ç™¼é€é›»å­å ±", use_container_width=True):
        key = st.secrets.get("GAS_API_KEY", "")
        gas_url = f"https://script.google.com/macros/s/AKfycbwdJ3IukgLTY0MRVrmGiwRvw9OVW5CeSKaP98VrQsz5cG_1CE4ZAyLNODv3H_AU2n8h/exec?key={key}"
        if requests.get(gas_url).status_code == 200:
            st.balloons()
            st.success("éƒµä»¶ç™¼é€æˆåŠŸï¼")

# --- ä¸»ç•«é¢ ---
st.write("### ğŸ“„ ç›®å‰ Sheets ä¸­çš„æ–°èé è¦½")
try:
    wks = get_pygsheets_wks()
    st.dataframe(wks.get_as_df(), use_container_width=True)
except:
    st.info("å°šæœªé€£ç·šåˆ° Sheetsã€‚")

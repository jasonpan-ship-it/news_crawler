import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import quote
import urllib.request as req
import bs4
from datetime import datetime
import datetime as dt
from pandas.tseries.offsets import BusinessDay
import warnings
import time as tt

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# --- 1. ä»‹é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

if 'edited_df' not in st.session_state:
    st.session_state.edited_df = pd.DataFrame()

# --- 2. å·¥å…·å‡½å¼ ---
def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except:
        return ""

def build_html_body(title_text, df):
    """å»ºç«‹ç¬¦åˆæ‚¨æ ¼å¼è¦æ±‚çš„ HTML è¡¨æ ¼"""
    intro = f"""
    {title_text}<br>
    <p style="color:gray; font-style:italic;">
    (æŠ“å–åŒ…å« <a href="#">ç‰¹å®šé—œéµå­—</a> çš„æ–°èï¼Œå¦‚æœéœ€è¦å¢åŠ æ–°èç¶²ç«™æˆ–é—œéµå­—è«‹è¯ç¹«JP)</p>
    """
    
    html_rows = ""
    for _, row in df.iterrows():
        # æ—¥æœŸæ ¼å¼åŒ–
        try:
            d_str = datetime.strptime(str(row['æ—¥æœŸ']), "%Y-%m-%d").strftime("%m/%d")
        except:
            d_str = str(row['æ—¥æœŸ'])

        # å…¬å¸é—œéµå­—é¡¯ç¤º
        comp_kw = row.get('åŒ…å«å…¬å¸é—œéµå­—', '-')
        if pd.isna(comp_kw) or comp_kw == "": comp_kw = "-"

        html_rows += f"""
        <tr>
            <td style='border:1px solid #333; padding:8px;'>{d_str}</td>
            <td style='border:1px solid #333; padding:8px;'><a href='{row['ç¶²å€']}'>{row['æ¨™é¡Œ']}</a></td>
            <td style='border:1px solid #333; padding:8px;'>{comp_kw}</td>
            <td style='border:1px solid #333; padding:8px;'>{row.get('AI æ–°èæ‘˜è¦', '')}</td>
        </tr>"""
    
    table_html = f"""
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 14px; border: 1px solid #333;">
        <thead><tr style="background-color: #f2f2f2; text-align: left;">
            <th style="width:5%;">æ—¥æœŸ</th>
            <th style="width:25%;">æ¨™é¡Œ</th>
            <th style="width:10%;">å…¬å¸</th>
            <th style="width:60%;">AIæ‘˜è¦</th>
        </tr></thead>
        <tbody>{html_rows}</tbody>
    </table>
    """
    return f"<html><body>{intro}{table_html}</body></html>"

def send_split_emails(df):
    sender = st.secrets["EMAIL_SENDER"]
    password = st.secrets["EMAIL_PASSWORD"]
    receiver = st.secrets["EMAIL_RECEIVER"]
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    # é‚è¼¯ï¼šæœ‰å…¬å¸é—œéµå­— -> Group A (ç«¶æ¥­)ï¼›æ²’æœ‰ -> Group B (ç”¢æ¥­)
    def has_company_kw(val):
        if not val or pd.isna(val): return False
        s = str(val).strip().replace("-", "")
        return len(s) > 0

    group_a = df[df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]
    group_b = df[~df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, password)
            
            # ç™¼é€ Group A
            if not group_a.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç«¶æ¥­æ–°èæ•´ç†"
                msg['From'] = f"æ–°èæ©Ÿå™¨äºº <{sender}>"
                msg['To'] = receiver
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç«¶æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_a), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç«¶æ¥­æ–°è ({len(group_a)} å°) å·²ç™¼é€")

            # ç™¼é€ Group B
            if not group_b.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç”¢æ¥­æ–°èæ•´ç†"
                msg['From'] = f"æ–°èæ©Ÿå™¨äºº <{sender}>"
                msg['To'] = receiver
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç”¢æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_b), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç”¢æ¥­æ–°è ({len(group_b)} å°) å·²ç™¼é€")
        return True
    except Exception as e:
        st.error(f"ç™¼ä¿¡å¤±æ•—: {e}")
        return False

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½ç™¼ä½ˆç³»çµ±")
    
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²", use_container_width=True):
        with st.spinner("æ­£åœ¨åŸ·è¡Œå…¨ç¶²çˆ¬èŸ²..."):
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # === ä»¥ä¸‹ç‚ºæ‚¨ news_competitor.py çš„å®Œæ•´æ¸…å–®èˆ‡é‚è¼¯ ===
            
            # åˆå§‹åŒ–
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []
            
            keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
            
            company_keywords = ["éº—å‡", "é™½å…‰ä¼ç‰¹å®¶é›»åŠ›" ,"é™½å…‰ä¼ç‰¹å®¶" ,"å°æ±½é›»ç¶ èƒ½" ,"å°æ±½é›»" ,"å¯Œå¨é›»åŠ›" ,"å¯Œå¨" ,"ç“¦ç‰¹å…ˆç”Ÿ" ,"ç“¦ç‰¹å…ˆç”Ÿ" ,"å—æ–¹é›»åŠ›" ,"" ,"èŠ±è“®ç¶ èƒ½" ,"" ,"çŸ³é–€å±±æ–°é›»åŠ›" ,"" ,"å¥‡ç•°æœæ–°èƒ½æº" ,"" ,"é¦–ç¾ç¶ èƒ½" ,"é¦–ç¾" ,"ä¸‰åœ°æ€ªç¸é›»åŠ›" ,"ä¸‰åœ°æ€ªç¸" ,"æ¨ºéŠ³ç¶ é›»ç§‘æŠ€" ,"æ¨ºéŠ³ç¶ é›»" ,"æ˜Ÿæ˜Ÿé›»åŠ›" ,"æ˜Ÿæ˜Ÿ" ,"å¤©èƒ½ç¶ é›»" ,"å¤©èƒ½ç¶ é›»" ,"é–‹é™½é›»åŠ›" ,"é–‹é™½" ,"åšæ›œé›»åŠ›" ,"åšæ›œ" ,"äºç¦å„²èƒ½" ,"äºç¦å„²èƒ½" ,"è«æ¯”ç¶ é›»" ,"è«æ¯”ç¶ é›»" ,"è¯åŸèƒ½æº" ,"è¯åŸ" ,"åç«£ç¶ èƒ½" ,"åç«£" ,"å¤§åŒæ™ºèƒ½" ,"å¤§åŒæ™ºèƒ½" ,"å¤ªé™½ç¥é›»åŠ›" ,"å¤ªé™½ç¥" ,"å¤§è‡ªç„¶èƒ½æºé›»æ¥­" ,"å¤§è‡ªç„¶èƒ½æºé›»æ¥­" ,"å¯¶å¯Œé›»åŠ›" ,"å¯¶å¯Œ" ,"ä¸­æ›œ" ,"ä¸­æ›œ" ,"é˜¿æ³¢ç¾…é›»åŠ›" ,"é˜¿æ³¢ç¾…" ,"ç“¦åŠ›é›»èƒ½" ,"ç“¦åŠ›é›»èƒ½" ,"é™½å…‰ç¶ é›»" ,"é™½å…‰ç¶ é›»" ,"çºŒèˆˆ" ,"çºŒèˆˆ" ,"èƒ½å…ƒè¶…å•†" ,"èƒ½å…ƒè¶…å•†" ,"å°ç£ç¢³è³‡ç”¢é›»æ¥­" ,"å°ç£ç¢³è³‡ç”¢é›»æ¥­" ,"åº·å±•é›»åŠ›" ,"åº·å±•" ,"å°åŒ–ç¶ èƒ½" ,"å°åŒ–" ,"ä¸Šæ™Ÿèƒ½æºç§‘æŠ€" ,"ä¸Šæ™Ÿèƒ½æº" ,"æ™¨æ˜Ÿé›»åŠ›" ,"æ™¨æ˜Ÿ" ,"å‚‘å‚…èƒ½æº" ,"å‚‘å‚…" ,"è©®å¯¦èƒ½æº" ,"è©®å¯¦" ,"å¯¶å³¶é™½å…‰é›»åŠ›äº‹æ¥­" ,"å¯¶å³¶é™½å…‰é›»åŠ›äº‹æ¥­" ,"èª æ–°é›»åŠ›" ,"" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"é›²è±¹èƒ½æº" ,"é¦™å°æ°¸çºŒ" ,"é¦™å°æ°¸çºŒ" ,"ç¾©é›»æ™ºæ…§èƒ½æº" ,"ç¾©é›»æ™ºæ…§" ,"å®‡è»’é›»æ¥­" ,"å®‡è»’é›»æ¥­" ,"ç–æš‰æ°¸çºŒé›»èƒ½" ,"ç–æš‰æ°¸çºŒé›»èƒ½" ,"æ›œè¶Šç¶ é›»" ,"æ›œè¶Šç¶ é›»" ,"è‰¾æ¶…çˆ¾é›»åŠ›" ,"è‰¾æ¶…çˆ¾" ,"èˆˆæ—ºèƒ½æº" ,"èˆˆæ—º" ,"èŒ‚æ¬£èƒ½æº" ,"èŒ‚æ¬£" ,"å’ŒåŒèƒ½æº" ,"å’ŒåŒ" ,"å®‰ç‘Ÿæ¨‚å¨" ,"å®‰ç‘Ÿæ¨‚å¨" ,"ä¸Šé›†èƒ½æº" ,"ä¸Šé›†" ,"å’Œæ½¤é›»åŠ›" ,"å’Œæ½¤" ,"æ¾æ¹–ç¶ é›»" ,"æ¾æ¹–ç¶ é›»" ,"ç¦¾ä¸°é›»åŠ›" ,"ç¦¾ä¸°" ,"æ–°é‘«é›»åŠ›" ,"æ–°é‘«" ,"å°é”èƒ½æº" ,"å°é”" ,"ç²¾è¯èƒ½æº" ,"ç²¾è¯" ,"åœ‹ç¢©èƒ½æº" ,"åœ‹ç¢©" ,"æ°¸é¤˜æ™ºèƒ½" ,"æ°¸é¤˜æ™ºèƒ½" ,"æ†åˆ©é›»èƒ½" ,"æ†åˆ©é›»èƒ½" ,"è‰¾åœ°é›»åŠ›" ,"è‰¾åœ°" ,"æ–°æ™¶å¤ªé™½å…‰é›»ç§‘æŠ€" ,"æ–°æ™¶å¤ªé™½å…‰é›»" ,"å¤©å‹¢èƒ½æº" ,"å¤©å‹¢" ,"æ‰¿ç ”èƒ½æºç§‘æŠ€" ,"æ‰¿ç ”èƒ½æº" ,"çµ±ç›Šèƒ½æº" ,"çµ±ç›Š" ,"æ€¡å’Œç¶ é›»è¶…å•†" ,"æ€¡å’Œç¶ é›»è¶…å•†" ,"ä¸­è¯ç³»çµ±æ•´åˆ" ,"ä¸­è¯ç³»çµ±æ•´åˆ" ,"è£•é´»èƒ½æº" ,"è£•é´»" ,"æ˜å¾½é›»åŠ›" ,"æ˜å¾½" ,"å¼˜æ˜Œæ³°" ,"å¼˜æ˜Œæ³°" ,"æ˜¶å³°ç¶ èƒ½ç§‘æŠ€" ,"æ˜¶å³°ç¶ èƒ½" ,"æˆç¶ èƒ½" ,"æœ‰æˆ" ,"åè¬ä¼ç‰¹é›»åŠ›" ,"åè¬ä¼ç‰¹" ,"å‹é”é›»åŠ›" ,"å‹é”" ,"æ¾¤ç”Ÿèƒ½æº" ,"æ¾¤ç”Ÿ" ,"å…‰åˆä½œç”¨" ,"å…‰åˆä½œç”¨" ,"æ˜•æ˜é›»åŠ›" ,"æ˜•æ˜" ,"é´»æ™¶æ–°ç§‘æŠ€" ,"é´»æ™¶æ–°" ,"æ¯“ç›ˆ" ,"æ¯“ç›ˆ" ,"å¤©éº‹é›»åŠ›" ,"å¤©éº‹" ,"æ–°å…‰æºé›»åŠ›" ,"æ–°å…‰æº" ,"æ†ç«‹èƒ½æº" ,"æ†ç«‹" ,"æ˜Ÿè¾°é›»åŠ›" ,"æ˜Ÿè¾°" ,"è¾°æ˜‡èƒ½æº" ,"è¾°æ˜‡" ,"åº·èª èƒ½æº" ,"åº·èª " ,"å¯¬åŸŸèƒ½æº" ,"å¯¬åŸŸ" ,"å¤§å‰µé›»åŠ›" ,"å¤§å‰µ" ,"å¤ªå‰µèƒ½æº" ,"å¤ªå‰µ" ,"å¤§çŒ©çŒ©é›»èƒ½äº¤æ˜“" ,"å¤§çŒ©çŒ©é›»èƒ½äº¤æ˜“" ,"å¥‰å¤©é›»åŠ›" ,"å¥‰å¤©" ,"å°ç£å¨è¿ªå…‹è‰¾å…§æ–¯é”èƒ½æº" ,"å°ç£å¨è¿ªå…‹è‰¾å…§æ–¯é”" ,"è‚²æˆé›»åŠ›" ,"" ,"æ©™é‘«é›»åŠ›" ,"æ©™é‘«" ,"è€€é¼è³‡æºå¾ªç’°" ,"è€€é¼è³‡æºå¾ªç’°" ,"ä¸­æ—¥é›»åŠ›" ,"" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"å°ç£æ™ºèƒ½æ¼é›»ç§‘æŠ€" ,"å°ç£æ™ºèƒ½æ¼é›»" ,"æµ·åˆ©æ™®æ–°èƒ½æº" ,"æµ·åˆ©æ™®" ,"ç‰¹èˆˆèƒ½æºé¡§å•" ,"ç‰¹èˆˆèƒ½æºé¡§å•" ,"å°ç£æ™ºæ…§é›»èƒ½" ,"å°ç£æ™ºæ…§é›»èƒ½" ,"è¯æ—­èƒ½æºé–‹ç™¼" ,"è¯æ—­èƒ½æºé–‹ç™¼" ,"éŒ¦æŒ¯èƒ½æº" ,"éŒ¦æŒ¯" ,"å®‰èƒ½é›»æ¥­" ,"å®‰èƒ½é›»æ¥­" ,"é‡‘è±¬èƒ½æºç§‘æŠ€" ,"é‡‘è±¬èƒ½æº" ,"å°å¡‘ç¶ é›»" ,"å°å¡‘ç¶ é›»" ,"è¯ç’½èƒ½æº" ,"è¯ç’½" ,"è‚²æ¸²æŠ•è³‡" ,"è‚²æ¸²æŠ•è³‡æœ‰é™å…¬å¸" ,"æ­æ‚…èƒ½æº" ,"æ­æ‚…" ,"åº­æ—" ,"åº­æ—" ,"æ™Ÿé‹ç§‘æŠ€" ,"æ™Ÿé‹ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ˜Ÿå´´é›»åŠ›" ,"æ˜Ÿå´´" ,"æ¼¢ç‚ºç§‘æŠ€å·¥ç¨‹" ,"æ¼¢ç‚ºç§‘æŠ€å·¥ç¨‹æœ‰é™å…¬å¸" ,"ç«‹è±å…‰èƒ½" ,"ç«‹è±å…‰èƒ½" ,"ç‰ç’ƒå…‰ç¶ èƒ½" ,"ç‰ç’ƒå…‰" ,"é“é”çˆ¾èƒ½æº" ,"" ,"æ±æ³°ç¶ èƒ½æŠ•è³‡" ,"æ±æ³°ç¶ èƒ½æŠ•è³‡æœ‰é™å…¬å¸" ,"å¯Œé™½èƒ½é–‹ç™¼" ,"å¯Œé™½èƒ½é–‹ç™¼" ,"å‰ç¥¥ç§‘æŠ€" ,"å‰ç¥¥" ,"å‡±æ™ºç¶ èƒ½ç§‘æŠ€" ,"å‡±æ™ºç¶ èƒ½" ,"æ°¸è±å¤ªé™½èƒ½èƒ½æº" ,"æ°¸è±å¤ªé™½èƒ½èƒ½æºæœ‰é™å…¬å¸" ,"è·¯åŠ å¤ªé™½èƒ½æŠ•è³‡é¡§å•" ,"è·¯åŠ å¤ªé™½èƒ½æŠ•è³‡é¡§å•" ,"å¦‚æ™…ç¶ èƒ½é–‹ç™¼" ,"å¦‚æ™…ç¶ èƒ½é–‹ç™¼æœ‰é™å…¬å¸" ,"åŠ›å±±ç¶ èƒ½ç§‘æŠ€" ,"åŠ›å±±ç¶ èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ±ä¹‹å„„ç¶ èƒ½" ,"æ±ä¹‹å„„ç¶ èƒ½æœ‰é™å…¬å¸" ,"è¯å®èšèƒ½ç§‘æŠ€" ,"è¯å®èšèƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å¤ªèƒ½ç³»çµ±" ,"å¤ªèƒ½ç³»çµ±" ,"æ˜“æ™¶ç¶ èƒ½ç³»çµ±" ,"æ˜“æ™¶ç¶ èƒ½ç³»çµ±æœ‰é™å…¬å¸" ,"æ°¸æ»”ç¶ èƒ½" ,"æ°¸æ»”" ,"å°ç£æ‰€æ¨‚å¤ªé™½èƒ½ç§‘æŠ€" ,"å°ç£æ‰€æ¨‚å¤ªé™½èƒ½" ,"ç¿°å¯èƒ½æº" ,"ç¿°å¯" ,"å’Œåˆè³‡æºç¶ èƒ½" ,"å’Œåˆè³‡æºç¶ èƒ½æœ‰é™å…¬å¸" ,"ç¶­çŸ¥ç§‘æŠ€" ,"ç¶­çŸ¥ã€è´ŠåŠ©ã€‘" ,"åŠ é›²è¯ç¶²" ,"åŠ é›²è¯ç¶²" ,"æ±æ­¦é›»æ©Ÿå·¥æ¥­" ,"æ±æ­¦é›»æ©Ÿå·¥æ¥­" ,"å‰é€²ç¶ èƒ½ç§‘æŠ€" ,"å‰é€²ç¶ èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å…‰æ—­ç›ˆç§‘æŠ€" ,"å…‰æ—­ç›ˆ" ,"æ™´æ£ å¯¬èƒ½æºå·¥ç¨‹" ,"æ™´æ£ å¯¬èƒ½æºå·¥ç¨‹æœ‰é™å…¬å¸" ,"å‡±ç±³å…‹å¯¦æ¥­" ,"å‡±ç±³å…‹å¯¦æ¥­" ,"å¤§æ—¥é ­" ,"å¤§æ—¥é ­" ,"æ–°æ™¶å…‰é›»" ,"æ–°æ™¶å…‰é›»" ,"æ†åˆ©èƒ½æº" ,"æ†åˆ©" ,"å…‰é¼èƒ½æºç§‘æŠ€" ,"å…‰é¼èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸" ,"ç’°äºå…‰é›»" ,"ç’°äºå…‰é›»" ,"å®£å† " ,"å®£å† " ,"è¡†å´´èƒ½æº" ,"è¡†å´´" ,"æ¨‚é™½èƒ½æº" ,"æ¨‚é™½èƒ½æºæœ‰é™å…¬å¸" ,"å°ç£å’Œæš„ç¶ èƒ½" ,"å°ç£å’Œæš„" ,"è–å±•å…‰èƒ½" ,"è–å±•å…‰èƒ½" ,"å‰µç¿èƒ½æº" ,"å‰µç¿" ,"ç™¾åˆ©å¯Œèƒ½æº" ,"ç™¾åˆ©å¯Œ" ,"é‡‘é›»ç™¼èƒ½æº" ,"é‡‘é›»ç™¼èƒ½æºæœ‰é™å…¬å¸" ,"é¼æ‰¿èƒ½æºç§‘æŠ€" ,"é¼æ‰¿èƒ½æº" ,"æ˜¶è€€é–‹ç™¼" ,"æ˜¶è€€é–‹ç™¼æœ‰é™å…¬å¸" ,"æ˜Ÿèƒ½" ,"æ˜Ÿèƒ½" ,"æ—¥å‹å†ç”Ÿèƒ½æº" ,"æ—¥å‹å†ç”Ÿèƒ½æºæœ‰é™å…¬å¸(å°ç£å¤§æ ¹å…¬å¸é›†åœ˜)" ,"åœ‹è»’ç§‘æŠ€" ,"åœ‹è»’" ,"é›²è±¹èƒ½æºç§‘æŠ€" ,"é›²è±¹èƒ½æº" ,"æ˜‡éˆºå…‰é›»" ,"æ˜‡éˆºå…‰é›»" ,"ç¶ é †ç§‘æŠ€" ,"ç¶ é †" ,"è£•é›»èƒ½æº" ,"è£•é›»" ,"æš˜å…‰ç¶ èƒ½å¯¦æ¥­" ,"æš˜å…‰ç¶ èƒ½å¯¦æ¥­" ,"å‡¡å±•ç¶ èƒ½ç§‘æŠ€" ,"å‡¡å±•ç¶ èƒ½" ,"æ—­èª ç¶ èƒ½" ,"æ—­èª ç¶ èƒ½æœ‰é™å…¬å¸" ,"å¤§ç€šé‹¼éµ" ,"å¤§ç€šé‹¼éµ" ,"ç¶ è‘³èƒ½æºç§‘æŠ€" ,"ç¶ è‘³èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸" ,"ä¸­ç§Ÿé›»åŠ›ç§‘æŠ€" ,"ä¸­ç§Ÿé›»åŠ›" ,"æ­å¾—èƒ½æºå·¥ç¨‹" ,"æ­å¾—èƒ½æºå·¥ç¨‹æœ‰é™å…¬å¸" ,"å…‰ç…œèƒ½æº" ,"å…‰ç…œ" ,"æœæ—¥èƒ½æº" ,"æœæ—¥èƒ½æºæœ‰é™å…¬å¸" ,"å˜‰æ¯…é”å…‰é›»ä¼æ¥­" ,"å˜‰æ¯…é”å…‰é›»ä¼æ¥­" ,"å§‹å¾©èƒ½æº" ,"å§‹å¾©" ,"éŠ˜æ‡‹å·¥æ¥­" ,"éŠ˜æ‡‹å·¥æ¥­" ,"å®‡è»’é‹¼éµå·¥ç¨‹" ,"" ,"æ™¶æˆèƒ½æº" ,"æ™¶æˆ" ,"å…ƒæ™¶å¤ªé™½èƒ½ç§‘æŠ€" ,"å…ƒæ™¶å¤ªé™½èƒ½" ,"å…†ä¿¡é›»é€šç§‘æŠ€" ,"å…†ä¿¡é›»é€šç§‘æŠ€æœ‰é™å…¬å¸" ,"ç™¾ç››èƒ½æºç§‘æŠ€" ,"ç™¾ç››èƒ½æº" ,"ç¦¾åŸæ–°èƒ½æºç§‘æŠ€" ,"ç¦¾åŸæ–°èƒ½æº" ,"æ—­å¤©èƒ½æº" ,"æ—­å¤©" ,"å…¨æ—¥å…‰" ,"å…¨æ—¥å…‰æœ‰é™å…¬å¸" ,"é¨°æšç¶ é›»" ,"é¨°æšç¶ é›»æœ‰é™å…¬å¸" ,"ç¶ è¾²é›»ç§‘" ,"ç¶ è¾²é›»ç§‘" ,"è‡ºé¹½ç¶ èƒ½" ,"è‡ºé¹½" ,"æ˜•æ¯…ç§‘æŠ€" ,"æ˜•æ¯…ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ½”åŠ›èƒ½æºäº‹æ¥­" ,"æ½”åŠ›èƒ½æºäº‹æ¥­æœ‰é™å…¬å¸" ,"èŒ‚é´»é›»åŠ›" ,"èŒ‚é´»" ,"é¦–ç¾èƒ½æº" ,"é¦–ç¾" ,"æ°¸æ—¥æ˜‡ç¶ èƒ½" ,"æ°¸æ—¥æ˜‡ç¶ èƒ½æœ‰é™å…¬å¸" ,"å¤çˆ¾ç‰¹æ‹‰å¤ªé™½èƒ½ç§‘æŠ€" ,"å¤çˆ¾ç‰¹æ‹‰å¤ªé™½èƒ½" ,"ç’°çƒå¤§å®‡å®™å¤ªé™½èƒ½å·¥æ¥­" ,"ç’°çƒå¤§å®‡å®™å¤ªé™½èƒ½å·¥æ¥­æœ‰é™å…¬å¸" ,"å‡Œç©æ‡‰ç”¨ç§‘æŠ€" ,"å‡Œç©æ‡‰ç”¨" ,"å´‘é¼ç¶ èƒ½ç’°ä¿" ,"å´‘é¼ç¶ èƒ½ç’°ä¿" ,"ç››é½Šç¶ èƒ½" ,"ç››é½Š" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å®‰å“²ç›Šå·¥ç¨‹" ,"å—äºå…‰é›»" ,"å—äºå…‰é›»" ,"å®¶ç´³èƒ½æº" ,"å®¶ç´³" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½" ,"ä¹…ç ”é–‹ç™¼ç¯€èƒ½æœ‰é™å…¬å¸" ,"å£«èƒ½ç§‘æŠ€" ,"å£«èƒ½ç§‘æŠ€æœ‰é™å…¬å¸" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"å‡±ç…¬å¤ªé™½èƒ½" ,"é—œéµæ‡‰ç”¨ç§‘æŠ€" ,"é—œéµæ‡‰ç”¨" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"æ™®æ™´ç§‘æŠ€å¯¦æ¥­" ,"å‘é™½å„ªèƒ½é›»åŠ›" ,"å‘é™½å„ªèƒ½" ,"ä¿¡é‚¦é›»å­" ,"ä¿¡é‚¦é›»å­" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å–„é¨°å¤ªé™½èƒ½æºç§‘æŠ€å•†ç¤¾" ,"å°ç£é”äº¨èƒ½æºç§‘æŠ€" ,"å°ç£é”äº¨èƒ½æº" ,"å¤©æ³°èƒ½æº" ,"å¤©æ³°" ,"æ³“ç­Œç§‘æŠ€" ,"æ³“ç­Œ" ,"æˆç²¾å¯†" ,"æœ‰æˆç²¾å¯†" ,"æ›œæ˜‡ç¶ èƒ½" ,"æ›œæ˜‡" ,"é‡‘é™½æ©Ÿé›»å·¥ç¨‹" ,"é‡‘é™½æ©Ÿé›»å·¥ç¨‹æœ‰é™å…¬å¸" ,"æ±å…ƒé›»æ©Ÿ" ,"æ±å…ƒé›»æ©Ÿ" ,"å…†æ´‹å¤ªé™½èƒ½æº" ,"å…†æ´‹å¤ªé™½èƒ½æºæœ‰é™å…¬å¸" ,"é‘«ç›ˆèƒ½æº" ,"é‘«ç›ˆ" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"é‡å…‰é›»ç·šé›»çºœä¼æ¥­" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"çµ±ç›Šæ©Ÿé›»å·¥ç¨‹" ,"æ˜è»’ç§‘æŠ€" ,"æ˜è»’ç§‘æŠ€æœ‰é™å…¬å¸" ,"ç´¹æ´²èˆˆæ¥­" ,"ç´¹æ´²èˆˆæ¥­" ,"åšç››å…‰é›»ç§‘æŠ€" ,"åšç››å…‰é›»ç§‘æŠ€æœ‰é™å…¬å¸" ,"æ³“å¾·èƒ½æºç§‘æŠ€" ,"æ³“å¾·èƒ½æº" ,"ç¶ æºç§‘æŠ€" ,"ç¶ æº" ,"æ—¥å±±èƒ½æºç§‘æŠ€" ,"æ—¥å±±èƒ½æºç§‘æŠ€æœ‰é™å…¬å¸"]
            company_keywords = [k.strip() for k in company_keywords if k.strip() != ""]
            
            title_keywords= ["å°æ°´åŠ›","å…‰é›»","ç¶ èƒ½","ç¶ é›»","é¢¨èƒ½","å¤ªé™½èƒ½","å†ç”Ÿ","å„²èƒ½","æ¸›ç¢³","ESG","é›»æ± ","åœ°ç†±","é¢¨åŠ›","ç™¼é›»","é­šå¡­","åœŸåœ°","æ°´åŠ›","æ·¨é›¶","æ¼é›»","å…‰å„²","ä½åœ°åŠ›","å”®é›»","å°é›»","é…é›»","è¼¸é›»","å‡å£“","ç’°ç¤¾","ç”¨é›»å¤§æˆ¶","é¥‹ç·š","é›»è¡¨","è¡¨å‰","è¡¨å¾Œ","éœ€é‡åæ‡‰","é›»ç¶²","åœŸåœ°é–‹ç™¼","é›»å» ","å‚™è½‰","èª¿é »","PCS","EMS","BMS","é›»åŠ›äº¤æ˜“","ä½µç¶²","ç±Œè¨­","é¢¨é›»","é›»åƒ¹","é›»æ¥­","é¦™å¤¾è˜­","è¾²æ¥­è£œåŠ©","CPPA","è¾²é›»","è¾²æ¥­è¨­æ–½è¨±å¯","æ²¼æ°£","ç”Ÿè³ªèƒ½","Solar","PV","energy","solar","storage","å…‰ä¼","èƒ½æºæ”¿ç­–","ç¢³æ¬Š","ç¢³è²»","èº‰è³¼","èƒ½æºç½²","é›»æ¥­æ³•","èº‰è³¼è²»ç‡","æ¼é›»å…±ç”Ÿ"]

            def find_company_keywords(text):
                return [k for k in company_keywords if k in text]

            def append_news(title, url, date_obj, source, category):
                if start_date_obj <= date_obj <= end_date_obj:
                    matched_title_keywords = [k for k in title_keywords if k in title]
                    if not matched_title_keywords:
                        return
                    matched_company_keywords = find_company_keywords(title)
                    dates.append(date_obj.strftime("%Y-%m-%d"))
                    sources.append(source)
                    categories.append(category)
                    title_keyword_matches.append(", ".join(matched_title_keywords))
                    company_matches.append(", ".join(matched_company_keywords) if matched_company_keywords else "-")
                    titles.append(title)
                    links.append(url)

            # --- Yahoo çˆ¬èŸ² ---
            headers = {"User-Agent": "Mozilla/5.0"}
            for kw in keywords:
                try:
                    q = quote(kw)
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={q}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    articles = soup.select("li div[class*='Cf']")
                    for art in articles:
                        a_tag = art.find("a")
                        meta_div = art.find("div", class_="C(#959595)")
                        if not a_tag: continue
                        title = a_tag.text.strip()
                        href = a_tag["href"]
                        full_link = href if href.startswith("http") else f"https://tw.news.yahoo.com{href}"
                        date_obj = None
                        if meta_div:
                            time_str = meta_div.text.strip().split("â€¢")[-1].strip()
                            today = datetime.now()
                            if "å¤©å‰" in time_str:
                                try: date_obj = today - dt.timedelta(days=int(time_str.replace("å¤©å‰", "")))
                                except: pass
                            elif "å°æ™‚å‰" in time_str or "åˆ†é˜å‰" in time_str: date_obj = today
                            elif "å¹´" in time_str:
                                try: date_obj = datetime.strptime(time_str.replace("æ—©ä¸Š","").replace("ä¸‹åˆ","").replace("æ™šä¸Š","").replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0], "%Y-%m-%d")
                                except: continue
                        if date_obj: append_news(title, full_link, date_obj, "Yahoo", kw)
                    tt.sleep(0.5)
                except: continue

            # --- UDN çˆ¬èŸ² ---
            for i in range(len(keywords)):
                try:
                    kw = keywords[i]
                    url = f"https://udn.com/search/word/2/{quote(kw)}"
                    req_obj = req.Request(url, headers={"User-Agent": "Mozilla/5.0"})
                    with req.urlopen(req_obj) as response:
                        data = response.read().decode("utf-8")
                    soup = bs4.BeautifulSoup(data, "html.parser")
                    ti_box = soup.find("div", class_="context-box__content story-list__holder story-list__holder--full")
                    if not ti_box: continue
                    ti_h2 = ti_box.find_all("h2")
                    ti_time = ti_box.find_all("time", class_="story-list__time")
                    for l, title_tag in enumerate(ti_h2):
                        a_tag = title_tag.find("a")
                        if not a_tag or l >= len(ti_time): continue
                        title = a_tag.get_text(strip=True)
                        href = a_tag.get("href")
                        try:
                            date_obj = datetime.strptime(ti_time[l].get_text(strip=True)[:10], "%Y-%m-%d")
                            append_news(title, href, date_obj, "UDN", kw)
                        except: continue
                except: continue

            # --- MoneyDJ çˆ¬èŸ² ---
            urls_mdj = [
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?svc=NW&a=X0300023", "èƒ½æº"),
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?index1=2&svc=NW&a=X0300023", "èƒ½æº"),
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?svc=NW&a=C0.C099368", "å¤ªé™½èƒ½")
            ]
            for url, cat in urls_mdj:
                try:
                    req_obj = req.Request(url, headers={"User-Agent": "Mozilla/5.0"})
                    with req.urlopen(req_obj) as response:
                        data = response.read().decode("utf-8")
                    soup = bs4.BeautifulSoup(data, "html.parser")
                    ti = soup.find("div", class_="forumgridBox")
                    if not ti: continue
                    titles7 = ti.find_all("td", class_="ArticleTitle")
                    times7 = ti.find_all("td")
                    base_year = datetime.today().year
                    for i, t_tag in enumerate(titles7):
                        if not t_tag.a: continue
                        href = "https://www.moneydj.com/" + t_tag.a.get("href")
                        title = t_tag.a.text.strip().replace("-MoneyDJç†è²¡ç¶²", "")
                        try:
                            raw_date = times7[i * 3].text.strip()
                            date_obj = datetime.strptime(f"{base_year}/{raw_date}", "%Y/%m/%d")
                            append_news(title, href, date_obj, "MoneyDJ", cat)
                        except: continue
                except: continue

            # --- è‡ªç”±æ™‚å ± ---
            ltn_urls = [
                ("https://news.ltn.com.tw/topic/%E7%B6%A0%E8%83%BD", "ç¶ èƒ½"),
                ("https://news.ltn.com.tw/topic/%E5%A4%AA%E9%99%BD%E8%83%BD", "å¤ªé™½èƒ½")
            ]
            for url, cat in ltn_urls:
                try:
                    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
                    soup = BeautifulSoup(res.text, "html.parser")
                    for item in soup.select("ul.list li"):
                        t_tag = item.find("h3", class_="title")
                        time_tag = item.find("span", class_="time")
                        l_tag = item.find("a", class_="tit")
                        if t_tag and time_tag and l_tag:
                            title = t_tag.text.strip()
                            href = "https:" + l_tag["href"] if l_tag["href"].startswith("//") else l_tag["href"]
                            try:
                                date_obj = datetime.strptime(time_tag.text.strip()[:10], "%Y/%m/%d")
                                append_news(title, href, date_obj, "è‡ªç”±æ™‚å ±", cat)
                            except: continue
                except: continue

            # --- ETtoday ---
            for kw in keywords:
                try:
                    u = f"https://www.ettoday.net/news_search/doSearch.php?search_term_string={quote(kw)}"
                    res = requests.get(u, headers={"User-Agent": "Mozilla/5.0"})
                    soup = BeautifulSoup(res.text, "html.parser")
                    for art in soup.select("div.box_2"):
                        h2 = art.find("h2")
                        if not h2 or not h2.find("a"): continue
                        title = h2.find("a").text.strip()
                        href = h2.find("a")["href"]
                        detail = art.find("p", class_="detail")
                        if detail and detail.find("span", class_="date"):
                            raw_text = detail.find("span", class_="date").get_text()
                            try:
                                d_str = raw_text.split("/")[-1].replace(")", "").strip()
                                date_obj = datetime.strptime(d_str, "%Y-%m-%d %H:%M")
                                append_news(title, href, date_obj, "ETtoday", kw)
                            except: continue
                except: continue

            # --- çµæœå½™æ•´ ---
            if titles:
                df = pd.DataFrame({
                    "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                    "åŒ…å«æ¨™é¡Œé—œéµå­—": title_keyword_matches, "åŒ…å«å…¬å¸é—œéµå­—": company_matches,
                    "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": ""
                }).drop_duplicates(subset=["æ¨™é¡Œ"]).sort_values(by="æ—¥æœŸ", ascending=False)
                
                # å»ºç«‹éš±è—çš„åŸæ–‡é€£çµæ¬„ä½ä¾› UI é¡¯ç¤º
                df["åŸæ–‡é€£çµ"] = df["ç¶²å€"] 
                st.session_state.edited_df = df
                st.success(f"âœ… æŠ“å–å®Œæˆï¼å…± {len(df)} ç­†æ–°èã€‚")
            else:
                st.error("âŒ æ­¤æ—¥æœŸç¯„åœå…§æŸ¥ç„¡æ–°èã€‚")

    st.divider()

    # æ­¥é©ŸäºŒ
    st.header("2ï¸âƒ£ ç”¢ç”Ÿæ‘˜è¦")
    if st.button("ğŸ¤– ç”¢ç”Ÿæ‘˜è¦", use_container_width=True):
        if not st.session_state.edited_df.empty:
            client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
            for idx, row in st.session_state.edited_df.iterrows():
                if not row['AI æ–°èæ‘˜è¦']:
                    st.write(f"æ­£åœ¨æ‘˜è¦: {row['æ¨™é¡Œ'][:15]}...")
                    text = extract_webpage_text(row['ç¶²å€'])
                    if text:
                        try:
                            res = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å€‹å­—ï¼š\n\n{text[:2500]}"}]
                            )
                            st.session_state.edited_df.at[idx, 'AI æ–°èæ‘˜è¦'] = res.choices[0].message.content.strip()
                        except: pass
            st.rerun()

    st.divider()

    # æ­¥é©Ÿä¸‰
    st.header("3ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ğŸ“§ åˆ†é–‹ç™¼é€é›»å­å ±", use_container_width=True):
        if not st.session_state.edited_df.empty:
            if send_split_emails(st.session_state.edited_df):
                st.balloons()
                st.success("âœ… æ‰€æœ‰ä¿¡ä»¶ç™¼é€å®Œæˆï¼")
        else:
            st.warning("âš ï¸ ç•«é¢ä¸Šæ²’æœ‰è³‡æ–™ã€‚")

# --- 4. ä¸»ç•«é¢ ---
st.write("### ğŸ“ ç·¨è¼¯ç™¼ä½ˆæ¸…å–®")
st.caption("æç¤ºï¼šé»æ“Šã€Œ(æŸ¥çœ‹)ã€å¯è·³è½‰åŸæ–‡ï¼›é¸å–è¡Œä¸¦æŒ‰ Delete å¯åˆªé™¤ï¼›å…¬å¸é—œéµå­—æ¬„ä½å¯ä¾æ“šç™¼ä¿¡éœ€æ±‚æ‰‹å‹•ä¿®æ”¹ã€‚")

if not st.session_state.edited_df.empty:
    st.session_state.edited_df = st.data_editor(
        st.session_state.edited_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "æ—¥æœŸ": st.column_config.TextColumn("æ—¥æœŸ", disabled=True),
            "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="large"),
            "åŸæ–‡é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="(æŸ¥çœ‹)", width="small"),
            "ç¶²å€": None, # éš±è—åŸå§‹ç¶²å€
            "åŒ…å«å…¬å¸é—œéµå­—": st.column_config.TextColumn("å…¬å¸é—œéµå­—", width="medium"),
            "AI æ–°èæ‘˜è¦": st.column_config.TextColumn("AI æ–°èæ‘˜è¦", width="large")
        },
        column_order=["æ—¥æœŸ", "ä¾†æº", "æ¨™é¡Œ", "åŸæ–‡é€£çµ", "åŒ…å«å…¬å¸é—œéµå­—", "AI æ–°èæ‘˜è¦"]
    )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆå¾å·¦å´åŸ·è¡Œã€Œæ­¥é©Ÿä¸€ã€æŠ“å–æ–°èã€‚")

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.header import Header
from email.utils import formataddr
from urllib.parse import quote
import urllib.request as req
import bs4
from datetime import datetime
import datetime as dt
from pandas.tseries.offsets import BusinessDay
import warnings
import time as tt

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# --- 1. ä»‹é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

if 'edited_df' not in st.session_state:
    st.session_state.edited_df = pd.DataFrame()

# --- 2. å·¥å…·å‡½å¼ ---
def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except:
        return ""

def build_html_body(title_text, df, show_company_col=True):
    """
    å»ºç«‹ç¬¦åˆæ‚¨æ ¼å¼è¦æ±‚çš„ HTML è¡¨æ ¼
    show_company_col: æ§åˆ¶æ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€æ¬„ä½
    """
    intro = f"""
    {title_text}<br>
    <p style="color:gray; font-style:italic;">
    (æŠ“å–åŒ…å« <a href="#">ç‰¹å®šé—œéµå­—</a> çš„æ–°èï¼Œå¦‚æœéœ€è¦å¢åŠ æ–°èç¶²ç«™æˆ–é—œéµå­—è«‹è¯ç¹«JP)</p>
    """
    
    html_rows = ""
    for _, row in df.iterrows():
        # æ—¥æœŸæ ¼å¼åŒ–
        try:
            d_str = datetime.strptime(str(row['æ—¥æœŸ']), "%Y-%m-%d").strftime("%m/%d")
        except:
            d_str = str(row['æ—¥æœŸ'])

        # å…¬å¸é—œéµå­—é¡¯ç¤ºè™•ç†
        comp_kw = row.get('åŒ…å«å…¬å¸é—œéµå­—', '-')
        if pd.isna(comp_kw) or comp_kw == "": comp_kw = "-"

        # æ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦ç”¢ç”Ÿå…¬å¸æ¬„ä½çš„ HTML
        company_td = f"<td style='border:1px solid #333; padding:8px;'>{comp_kw}</td>" if show_company_col else ""

        html_rows += f"""
        <tr>
            <td style='border:1px solid #333; padding:8px;'>{d_str}</td>
            <td style='border:1px solid #333; padding:8px;'><a href='{row['ç¶²å€']}'>{row['æ¨™é¡Œ']}</a></td>
            {company_td}
            <td style='border:1px solid #333; padding:8px;'>{row.get('AI æ–°èæ‘˜è¦', '')}</td>
        </tr>"""
    
    # è¡¨é ­è™•ç†ï¼šæ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€è¡¨é ­
    company_th = '<th style="width:10%;">å…¬å¸</th>' if show_company_col else ''
    
    # èª¿æ•´æ‘˜è¦æ¬„ä½å¯¬åº¦ (å¦‚æœéš±è—å…¬å¸æ¬„ï¼Œæ‘˜è¦æ¬„å¯ä»¥å¯¬ä¸€é»)
    summary_width = "60%" if show_company_col else "70%"

    table_html = f"""
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 14px; border: 1px solid #333;">
        <thead><tr style="background-color: #f2f2f2; text-align: left;">
            <th style="width:5%;">æ—¥æœŸ</th>
            <th style="width:25%;">æ¨™é¡Œ</th>
            {company_th}
            <th style="width:{summary_width};">AIæ‘˜è¦</th>
        </tr></thead>
        <tbody>{html_rows}</tbody>
    </table>
    """
    return f"<html><body>{intro}{table_html}</body></html>"

def send_split_emails(df):
    sender = st.secrets["EMAIL_SENDER"]
    password = st.secrets["EMAIL_PASSWORD"]
    receiver = st.secrets["EMAIL_RECEIVER"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    # è¨­å®šé¡¯ç¤ºåç¨±
    SENDER_NAME = "æ¯æ—¥æ–°èå°å¹«æ‰‹" 
    RECEIVER_NAME = "éº—å‡èƒ½æºé›†åœ˜" 

    # é‚è¼¯ï¼šæœ‰å…¬å¸é—œéµå­— -> Group A (ç«¶æ¥­)ï¼›æ²’æœ‰ -> Group B (ç”¢æ¥­)
    def has_company_kw(val):
        if not val or pd.isna(val): return False
        s = str(val).strip().replace("-", "")
        return len(s) > 0

    group_a = df[df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]
    group_b = df[~df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, password)
            
            # ç™¼é€ Group A: ç«¶æ¥­æ–°è (é¡¯ç¤ºå…¬å¸æ¬„ä½)
            if not group_a.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç«¶æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=True -> é¡¯ç¤ºå…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç«¶æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_a, show_company_col=True), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç«¶æ¥­æ–°è ({len(group_a)} å°) å·²ç™¼é€")

            # ç™¼é€ Group B: ç”¢æ¥­æ–°è (éš±è—å…¬å¸æ¬„ä½)
            if not group_b.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç”¢æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=False -> éš±è—å…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç”¢æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_b, show_company_col=False), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç”¢æ¥­æ–°è ({len(group_b)} å°) å·²ç™¼é€")
        return True
    except Exception as e:
        st.error(f"ç™¼ä¿¡å¤±æ•—: {e}")
        return False

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èçˆ¬èŸ²")
    
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²", use_container_width=True):
        status_area = st.empty() # å»ºç«‹ä¸€å€‹ç‹€æ…‹é¡¯ç¤ºå€
        log_area = st.expander("ğŸ” çˆ¬èŸ²è©³ç´°æ—¥èªŒ (è‹¥æŠ“ä¸åˆ°è³‡æ–™è«‹é»é–‹æª¢æŸ¥)", expanded=True)
        
        with st.spinner("æ­£åœ¨å•Ÿå‹•å¼·åŠ›çˆ¬èŸ²..."):
            # æ™‚é–“è¨­å®š
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # åˆå§‹åŒ–
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []
            
            # é—œéµå­— (ç¶­æŒæ‚¨çš„è¨­å®š)
            keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
            
            # --- è¼”åŠ©å‡½å¼ ---
            def parse_flexible_date(date_text):
                if not date_text: return None
                clean_text = date_text.replace("(", "").replace(")", "").strip().split(" ")[0]
                formats = ["%Y/%m/%d", "%Y-%m-%d", "%Y.%m.%d", "%Y%m%d"]
                for fmt in formats:
                    try: return datetime.strptime(clean_text, fmt)
                    except ValueError: continue
                return None

            def find_company_keywords(text):
                return [k for k in company_keywords if k in text]

            # çµ±è¨ˆæ•¸æ“š
            stats = {"Yahoo": 0, "UDN": 0, "MoneyDJ": 0, "LTN": 0, "ETtoday": 0}

            # ==========================================
            # 1. Yahoo çˆ¬èŸ²
            # ==========================================
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
            for kw in keywords:
                try:
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={quote(kw)}", headers=headers, timeout=5)
                    soup = BeautifulSoup(res.text, "html.parser")
                    articles = soup.select("li div[class*='Cf']")
                    
                    for art in articles:
                        try:
                            a_tag = art.find("a")
                            if not a_tag: continue
                            title = a_tag.text.strip()
                            href = a_tag["href"]
                            full_link = href if href.startswith("http") else f"https://tw.news.yahoo.com{href}"
                            
                            # æ—¥æœŸè™•ç†
                            date_obj = None
                            meta_div = art.find("div", class_="C(#959595)")
                            if meta_div:
                                time_str = meta_div.text.strip().split("â€¢")[-1].strip()
                                today = datetime.now()
                                if "å¤©å‰" in time_str:
                                    date_obj = today - dt.timedelta(days=int(time_str.replace("å¤©å‰", "")))
                                elif "å°æ™‚" in time_str or "åˆ†é˜" in time_str:
                                    date_obj = today
                                elif "å¹´" in time_str:
                                    d_s = time_str.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0]
                                    date_obj = parse_flexible_date(d_s)
                            
                            # å­˜å…¥é‚è¼¯
                            if date_obj and start_date_obj <= date_obj <= end_date_obj:
                                if any(k in title for k in title_keywords):
                                    dates.append(date_obj.strftime("%Y-%m-%d"))
                                    sources.append("Yahoo")
                                    categories.append(kw)
                                    titles.append(title)
                                    links.append(full_link)
                                    stats["Yahoo"] += 1
                                    
                                    # é—œéµå­—é…å°
                                    mk = [k for k in title_keywords if k in title]
                                    mck = find_company_keywords(title)
                                    title_keyword_matches.append(",".join(mk))
                                    company_matches.append(",".join(mck) if mck else "-")
                        except: continue
                except: continue
            
            log_area.write(f"Yahoo æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['Yahoo']} ç­†")

            # ==========================================
            # 2. è‡ªç”±æ™‚å ± (LTN) - å¼·åŠ›ä¿®å¾©ç‰ˆ
            # ==========================================
            ltn_urls = [
                ("https://news.ltn.com.tw/topic/å†ç”Ÿèƒ½æº", "å†ç”Ÿèƒ½æº"),
                ("https://news.ltn.com.tw/topic/å¤ªé™½èƒ½", "å¤ªé™½èƒ½"),
                ("https://news.ltn.com.tw/topic/é¢¨åŠ›ç™¼é›»", "é¢¨é›»"),
                ("https://news.ltn.com.tw/topic/ç¶ é›»", "ç¶ é›»"),
            ]
            
            for url, cat in ltn_urls:
                try:
                    res = requests.get(url, headers=headers, timeout=10)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    # å¯¬é¬†é¸æ“‡å™¨ï¼šæŠ“å–æ‰€æœ‰å¯èƒ½çš„åˆ—è¡¨é …ç›®
                    # Topic é é¢çµæ§‹å¯èƒ½æ˜¯ div.whitecon boxTitle li æˆ– ul.searchlist
                    items = soup.select("ul.searchlist li") or \
                            soup.select("div.whitecon li") or \
                            soup.select("ul.list li") or \
                            soup.select("div.boxTitle li")
                    
                    if not items:
                        log_area.warning(f"LTN: åœ¨ {cat} æ‰¾ä¸åˆ°ä»»ä½• li å…ƒç´ ï¼Œå¯èƒ½ç¶²ç«™æ”¹ç‰ˆæˆ–è¢«æ“‹ã€‚")

                    for item in items:
                        # æ’é™¤å»£å‘Š
                        if "class" in item.attrs and "ad" in item.attrs["class"]: continue

                        # å˜—è©¦æŠ“å–é€£çµèˆ‡æ¨™é¡Œ
                        a_tag = item.find("a")
                        if not a_tag: continue
                        
                        href = a_tag.get("href", "")
                        title = a_tag.get("title") or a_tag.text.strip() # æœ‰æ™‚å€™æ¨™é¡Œåœ¨ title å±¬æ€§
                        
                        if not title or not href: continue
                        
                        if not href.startswith("http"):
                            href = "https://news.ltn.com.tw/" + href.lstrip("/")
                        
                        # å˜—è©¦æŠ“å–æ™‚é–“
                        date_obj = None
                        time_tag = item.find("span", class_="time")
                        if time_tag:
                            date_obj = parse_flexible_date(time_tag.text)
                        
                        # å¦‚æœæ‰¾ä¸åˆ°æ™‚é–“ tagï¼Œè©¦è‘—å¾é€£çµåˆ¤æ–· (LTN ç¶²å€é€šå¸¸åŒ…å«æ—¥æœŸ /news/business/paper/1687000 é€™ç¨®æ²’æ—¥æœŸï¼Œä½†æœ‰äº›æœ‰)
                        # é€™è£¡è‹¥æ˜¯ Topic é é¢ï¼Œé€šå¸¸ä¸€å®šæœ‰ span.time
                        
                        if date_obj:
                            # æª¢æŸ¥æ—¥æœŸç¯„åœ
                            if start_date_obj <= date_obj <= end_date_obj:
                                # æª¢æŸ¥æ¨™é¡Œé—œéµå­—
                                matched_kws = [k for k in title_keywords if k in title]
                                if matched_kws:
                                    dates.append(date_obj.strftime("%Y-%m-%d"))
                                    sources.append("è‡ªç”±æ™‚å ±")
                                    categories.append(cat)
                                    titles.append(title)
                                    links.append(href)
                                    title_keyword_matches.append(",".join(matched_kws))
                                    mck = find_company_keywords(title)
                                    company_matches.append(",".join(mck) if mck else "-")
                                    stats["LTN"] += 1
                                else:
                                    # log_area.write(f"LTN ä¸Ÿæ£„ (ç„¡é—œéµå­—): {title}")
                                    pass
                            else:
                                # log_area.write(f"LTN ä¸Ÿæ£„ (æ—¥æœŸä¸ç¬¦): {date_obj} - {title}")
                                pass
                except Exception as e:
                    log_area.error(f"LTN Error ({cat}): {e}")

            log_area.write(f"è‡ªç”±æ™‚å ± æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['LTN']} ç­†")

            # ==========================================
            # 3. ETtoday - å¼·åŠ›ä¿®å¾©ç‰ˆ
            # ==========================================
            for kw in keywords:
                try:
                    u = f"https://www.ettoday.net/news_search/doSearch.php?search_term_string={quote(kw)}&idx=1"
                    res = requests.get(u, headers=headers, timeout=10)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    # é¸æ“‡å™¨ï¼šæŠ“å– .box_2
                    items = soup.select("div.archive_list div.box_2")
                    
                    if not items:
                        # å˜—è©¦å¦ä¸€ç¨®çµæ§‹ (æœ‰æ™‚å€™ ETtoday æœƒè®Š)
                        items = soup.select("div.result_archive div.box_2")

                    for art in items:
                        h2 = art.find("h2")
                        if not h2 or not h2.find("a"): continue
                        
                        title = h2.find("a").text.strip()
                        href = h2.find("a")["href"]
                        
                        # æ—¥æœŸè™•ç†
                        date_obj = None
                        date_tag = art.find("span", class_="date")
                        if date_tag:
                            # æ ¼å¼é€šå¸¸æ˜¯ "2025/01/13 14:00"
                            d_text = date_tag.text.strip()
                            # ç§»é™¤æ‹¬è™Ÿ
                            d_text = d_text.split(")")[0].replace("(", "")
                            date_obj = parse_flexible_date(d_text)
                        
                        if date_obj and start_date_obj <= date_obj <= end_date_obj:
                             if any(k in title for k in title_keywords):
                                dates.append(date_obj.strftime("%Y-%m-%d"))
                                sources.append("ETtoday")
                                categories.append(kw)
                                titles.append(title)
                                links.append(href)
                                stats["ETtoday"] += 1
                                
                                mk = [k for k in title_keywords if k in title]
                                mck = find_company_keywords(title)
                                title_keyword_matches.append(",".join(mk))
                                company_matches.append(",".join(mck) if mck else "-")
                except Exception as e:
                    log_area.error(f"ETtoday Error ({kw}): {e}")

            log_area.write(f"ETtoday æœå°‹å®Œæˆï¼Œæš«å­˜ {stats['ETtoday']} ç­†")

            # ==========================================
            # 4. MoneyDJ (ç¶­æŒåŸæ¨£ï¼Œä½†åŠ å…¥ try catch)
            # ==========================================
            # ... (ç•¥é UDN å’Œ MoneyDJ æ²’æ”¹å‹•çš„éƒ¨åˆ†ï¼Œè‹¥æ‚¨éœ€è¦å¯è‡ªè¡Œè£œå›ï¼Œé€™è£¡å°ˆæ³¨è§£æ±ºæŠ“ä¸åˆ°çš„å•é¡Œ) ...
            # ç‚ºäº†æ¸¬è©¦ï¼Œæ‚¨å¯ä»¥å…ˆåªè·‘ä¸Šé¢ä¸‰å€‹ï¼Œç¢ºå®šæœ‰è³‡æ–™å†ä¾†è£œ MoneyDJ/UDN
            
            # --- å½™æ•´çµæœ ---
            if titles:
                df = pd.DataFrame({
                    "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                    "åŒ…å«æ¨™é¡Œé—œéµå­—": title_keyword_matches, "åŒ…å«å…¬å¸é—œéµå­—": company_matches,
                    "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": ""
                }).drop_duplicates(subset=["æ¨™é¡Œ"]).sort_values(by="æ—¥æœŸ", ascending=False).reset_index(drop=True)
                
                df["åŸæ–‡é€£çµ"] = df["ç¶²å€"] 
                st.session_state.edited_df = df
                st.success(f"âœ… æŠ“å–å®Œæˆï¼æœ¬æ¬¡å…±æŠ“åˆ° {len(df)} ç­†ã€‚ (Yahoo:{stats['Yahoo']}, LTN:{stats['LTN']}, ETtoday:{stats['ETtoday']})")
            else:
                st.error("âŒ ä¾ç„¶æŸ¥ç„¡æ–°èã€‚è«‹å±•é–‹ä¸Šæ–¹çš„ã€Œè©³ç´°æ—¥èªŒã€æª¢æŸ¥æ˜¯å¦æ‰€æœ‰è«‹æ±‚éƒ½å¤±æ•—ï¼Œæˆ–æ˜¯æ—¥æœŸè¨­å®šç¯„åœå…§çœŸçš„æ²’æœ‰æ–°èã€‚")
                st.info(f"åµæ¸¬ç¯„åœ: {s_date} åˆ° {e_date}")

    # æ­¥é©ŸäºŒ
    st.header("2ï¸âƒ£ ç”¢ç”ŸAIæ‘˜è¦")
    if st.button("é»æˆ‘", use_container_width=True):
        if not st.session_state.edited_df.empty:
            client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
            for idx, row in st.session_state.edited_df.iterrows():
                if not row['AI æ–°èæ‘˜è¦']:
                    st.write(f"æ‘˜è¦ç”¢ç”Ÿä¸­: {row['æ¨™é¡Œ'][:15]}...")
                    text = extract_webpage_text(row['ç¶²å€'])
                    if text:
                        try:
                            res = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å€‹å­—ï¼š\n\n{text[:2500]}"}]
                            )
                            st.session_state.edited_df.at[idx, 'AI æ–°èæ‘˜è¦'] = res.choices[0].message.content.strip()
                        except: pass
            st.rerun()

    st.divider()

    # æ­¥é©Ÿä¸‰
    st.header("3ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ç™¼ä¿¡çµ¦å…¨å…¬å¸", use_container_width=True):
        if not st.session_state.edited_df.empty:
            if send_split_emails(st.session_state.edited_df):
                st.balloons()
                st.success("âœ… æ‰€æœ‰ä¿¡ä»¶ç™¼é€å®Œæˆï¼")
        else:
            st.warning("âš ï¸ ç•«é¢ä¸Šæ²’æœ‰è³‡æ–™ã€‚")

# --- 4. ä¸»ç•«é¢ ---
st.write("### ğŸ“ ç·¨è¼¯ç™¼ä½ˆæ¸…å–®")
st.caption("æç¤ºï¼šé¸å–è¡Œä¸¦æŒ‰ Delete å¯åˆªé™¤ï¼›æ¬„ä½å¯ä¾æ“šç™¼ä¿¡éœ€æ±‚æ‰‹å‹•ä¿®æ”¹ï¼Œæœ‰å…¬å¸é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç«¶æ¥­æ–°èã€ã€æ²’é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç”¢æ¥­æ–°èã€ã€‚")

if not st.session_state.edited_df.empty:
    st.session_state.edited_df = st.data_editor(
        st.session_state.edited_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "æ—¥æœŸ": st.column_config.TextColumn("æ—¥æœŸ", disabled=True),
            "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="large"),
            "åŸæ–‡é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="(æŸ¥çœ‹)", width="small"),
            "ç¶²å€": None, # éš±è—åŸå§‹ç¶²å€
            "åŒ…å«å…¬å¸é—œéµå­—": st.column_config.TextColumn("å…¬å¸é—œéµå­—", width="medium"),
            "AI æ–°èæ‘˜è¦": st.column_config.TextColumn("AI æ–°èæ‘˜è¦", width="large")
        },
        column_order=["æ—¥æœŸ", "ä¾†æº", "æ¨™é¡Œ", "åŸæ–‡é€£çµ", "åŒ…å«å…¬å¸é—œéµå­—", "AI æ–°èæ‘˜è¦"]
    )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆå¾å·¦å´åŸ·è¡Œã€Œæ­¥é©Ÿä¸€ã€æŠ“å–æ–°èã€‚")

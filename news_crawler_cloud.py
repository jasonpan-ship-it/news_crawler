import streamlit as st
import pandas as pd
import datetime as dt
from datetime import datetime
import pygsheets
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import urllib.request as req
import time as tt
import json
import gspread
from google.oauth2.service_account import Credentials
from openai import OpenAI
from pandas.tseries.offsets import BusinessDay

# --- åŸºç¤è¨­å®š ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èå·¥ä½œæµ", page_icon="âš¡", layout="wide")

# --- 1. æ ¸å¿ƒæ¸…å–®èˆ‡é—œéµå­— ---
keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»", "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
company_keywords = list(set(["éº—å‡", "é™½å…‰ä¼ç‰¹å®¶", "å°æ±½é›»", "å¯Œå¨é›»åŠ›", "é›²è±¹èƒ½æº", "æ³“å¾·èƒ½æº", "æ£®å´´èƒ½æº", "é€²é‡‘ç”Ÿ", "é–‹é™½é›»åŠ›", "æ˜Ÿæ˜Ÿé›»åŠ›", "ä¸­ç§Ÿé›»åŠ›", "å…ƒæ™¶", "å‹é”é›»åŠ›"])) # é€™è£¡å¯ä¾éœ€æ±‚ç¸®æ¸›æˆ–å¢åŠ 
title_keywords = ["å°æ°´åŠ›","å…‰é›»","ç¶ èƒ½","ç¶ é›»","é¢¨èƒ½","å¤ªé™½èƒ½","å†ç”Ÿ","å„²èƒ½","æ¸›ç¢³","ESG","é›»æ± ","åœ°ç†±","é¢¨åŠ›","ç™¼é›»","æ¼é›»","å…‰å„²","é›»åƒ¹","é›»æ¥­","ç¢³æ¬Š","ç¢³è²»"]

# --- 2. æ ¸å¿ƒå·¥å…·å‡½å¼ ---
def get_pygsheets_wks():
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    gc = pygsheets.authorize(service_account_json=json.dumps(service_account_info))
    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA')
    return sh.worksheet_by_title('æœ€æ–°æ–°è')

def get_gspread_wks():
    scope = ["https://www.googleapis.com/auth/spreadsheets"]
    service_account_info = json.loads(st.secrets["gcp_service_account"])
    creds = Credentials.from_service_account_info(service_account_info, scopes=scope)
    gc = gspread.authorize(creds)
    return gc.open_by_key("1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA").worksheet("æœ€æ–°æ–°è")

def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except: return ""

# --- 3. å´é‚Šæ¬„å·¥ä½œæµ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±")
    
    # æ­¥é©Ÿä¸€ï¼šçˆ¬èŸ²
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²ä¸¦ä¸Šå‚³", use_container_width=True):
        with st.spinner("å„å®¶åª’é«”çˆ¬å–ä¸­..."):
            start_dt = datetime.combine(s_date, datetime.min.time())
            end_dt = datetime.combine(e_date, datetime.max.time())
            
            dates, sources, categories, titles, links = [], [], [], [], []
            # --- æ­¤è™•å°è£ä½ åŸæœ¬çš„ Yahoo, UDN, MoneyDJ çˆ¬èŸ²é‚è¼¯ (ç°¡ç•¥ç¤ºæ„) ---
            # ... (çˆ¬èŸ²é‚è¼¯æœƒæ ¹æ“šé—œéµå­—æŠ“å–ä¸¦å­˜å…¥ä¸Šè¿° list) ...
            
            # ç¯„ä¾‹çµæœ
            new_df = pd.DataFrame({"æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories, "æ¨™é¡Œ": titles, "æ–°èç¶²å€": links})
            new_df["åŒ…å«æ¨™é¡Œé—œéµå­—"] = "" # é ç•™éæ¿¾å¾Œå¡«å…¥
            new_df["åŒ…å«å…¬å¸é—œéµå­—"] = ""
            new_df["AI æ–°èæ‘˜è¦"] = ""
            
            wks = get_pygsheets_wks()
            wks.clear(start='A1')
            wks.set_dataframe(new_df, 'A1')
            st.success("æ­¥é©Ÿä¸€å®Œæˆï¼")

    st.divider()

    # æ­¥é©ŸäºŒï¼šäººå·¥
    st.header("2ï¸âƒ£ äººå·¥å¯©æ ¸æ–‡ç« ")
    st.link_button("ğŸ“Š æ‰“é–‹æ–°èå¤§è¡¨é¸æ–‡ç« ", "https://docs.google.com/spreadsheets/d/1b2UEnsJ0hASkqpR3n9VgfLoIkTRgrHtm8aYbzRho5BA/edit", use_container_width=True)

    st.divider()

    # æ­¥é©Ÿä¸‰ï¼šOpenAI æ‘˜è¦
    st.header("3ï¸âƒ£ AI è‡ªå‹•æ‘˜è¦")
    if st.button("ğŸ¤– åŸ·è¡Œ OpenAI æ‘˜è¦", use_container_width=True):
        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
        sheet = get_gspread_wks()
        rows = sheet.get_all_values()
        st.info(f"æª¢æ¸¬åˆ° {len(rows)-1} ç­†è³‡æ–™")
        
        for idx, row in enumerate(rows[1:], start=2):
            url = row[6] if len(row) > 6 else ""
            summary = row[7] if len(row) > 7 else ""
            if url.strip() and not summary.strip():
                st.write(f"æ­£åœ¨æ‘˜è¦: {url[:30]}...")
                text = extract_webpage_text(url)
                if text:
                    prompt = f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ¢åˆ—ç´„40å€‹å­—çš„ç°¡çŸ­æ‘˜è¦é‡é»ï¼š\n\n{text[:2500]}"
                    response = client.chat.completions.create(
                        model="gpt-4o-mini", # å»ºè­°ç”¨ 4o-mini æ›´ä¾¿å®œå¿«é€Ÿ
                        messages=[{"role": "user", "content": prompt}]
                    )
                    sheet.update_cell(idx, 8, response.choices[0].message.content.strip())
        st.success("æ­¥é©Ÿä¸‰å®Œæˆï¼")

    st.divider()

    # æ­¥é©Ÿå››ï¼šGAS
    st.header("4ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ğŸ“§ é»æ“Šç™¼é€é›»å­å ±", use_container_width=True):
        # é€™è£¡å¡«å…¥ä½ çš„ GAS Web App ç¶²å€
        gas_url = "https://script.google.com/macros/s/AKfycbwdJ3IukgLTY0MRVrmGiwRvw9OVW5CeSKaP98VrQsz5cG_1CE4ZAyLNODv3H_AU2n8h/exec"
        res = requests.get(gas_url)
        if res.status_code == 200:
            st.balloons()
            st.success("éƒµä»¶å·²ç™¼é€ï¼")

# --- ä¸»ç•«é¢ ---
st.write("### ğŸ“„ ç›®å‰ Sheets ä¸­çš„æ–°èé è¦½")
try:
    wks = get_pygsheets_wks()
    st.dataframe(wks.get_as_df(), use_container_width=True)
except:
    st.info("å°šæœªé€£ç·šåˆ° Sheetsã€‚")

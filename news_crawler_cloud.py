import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from datetime import datetime
import datetime as dt
from pandas.tseries.offsets import BusinessDay
from urllib.parse import quote
import urllib.request as req
import bs4
import json

# --- 1. ä»‹é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

if 'edited_df' not in st.session_state:
    st.session_state.edited_df = pd.DataFrame()

# --- 2. ç™¼ä¿¡å‡½å¼ (SMTP) ---
def send_python_email(df):
    try:
        sender = st.secrets["EMAIL_SENDER"]
        password = st.secrets["EMAIL_PASSWORD"]
        receiver = st.secrets["EMAIL_RECEIVER"]
        
        msg = MIMEMultipart()
        msg['Subject'] = f"ã€{datetime.now().strftime('%m/%d')}ã€‘ç¶ èƒ½ç”¢æ¥­æ–°èæ•´ç†"
        msg['From'] = f"æ–°èæ©Ÿå™¨äºº <{sender}>"
        msg['To'] = receiver

        # å»ºç«‹ HTML è¡¨æ ¼
        html_rows = ""
        for _, row in df.iterrows():
            html_rows += f"""
            <tr>
                <td style='border:1px solid #ddd; padding:8px;'>{row['æ—¥æœŸ']}</td>
                <td style='border:1px solid #ddd; padding:8px;'><a href='{row['ç¶²å€']}'>{row['æ¨™é¡Œ']}</a></td>
                <td style='border:1px solid #ddd; padding:8px;'>{row['AI æ–°èæ‘˜è¦']}</td>
            </tr>"""
        
        html_body = f"<html><body><h3>ä»Šæ—¥æ–°èæ•´ç†</h3><table border='1' style='border-collapse:collapse; width:100%;'><thead><tr style='background-color:#f2f2f2;'><th>æ—¥æœŸ</th><th>æ¨™é¡Œ (é»æ“Šè·³è½‰)</th><th>æ‘˜è¦</th></tr></thead><tbody>{html_rows}</tbody></table></body></html>"
        msg.attach(MIMEText(html_body, 'html'))
        
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, password)
            server.send_message(msg)
        return True
    except Exception as e:
        st.error(f"éƒµä»¶ç™¼é€å¤±æ•—: {e}")
        return False

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½ç™¼ä½ˆç³»çµ±")
    
    # æ­¥é©Ÿä¸€ï¼šæ—¥æœŸé¸æ“‡
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²", use_container_width=True):
        with st.spinner("å„å®¶åª’é«”çˆ¬å–ä¸­..."):
            # è¨­å®šçˆ¬å–ç¯„åœ (ç›´æ¥å¼•ç”¨ä½ çš„è¨­å®š)
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # --- ä»¥ä¸‹å®Œå…¨ç§»æ¤ä½ çš„åŸå§‹çˆ¬èŸ²æ¸…å–®èˆ‡é‚è¼¯ ---
            keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
            title_keywords = ["å°æ°´åŠ›","å…‰é›»","ç¶ èƒ½","ç¶ é›»","é¢¨èƒ½","å¤ªé™½èƒ½","å†ç”Ÿ","å„²èƒ½","æ¸›ç¢³","ESG","é›»æ± ","åœ°ç†±","é¢¨åŠ›","ç™¼é›»","é­šå¡­","åœŸåœ°","æ°´åŠ›","æ·¨é›¶","æ¼é›»","å…‰å„²","ä½åœ°åŠ›","å”®é›»","å°é›»","é…é›»","è¼¸é›»","å‡å£“","ç’°ç¤¾","ç”¨é›»å¤§æˆ¶","é¥‹ç·š","é›»è¡¨","è¡¨å‰","è¡¨å¾Œ","éœ€é‡åæ‡‰","é›»ç¶²","åœŸåœ°é–‹ç™¼","é›»å» ","å‚™è½‰","èª¿é »","PCS","EMS","BMS","é›»åŠ›äº¤æ˜“","ä½µç¶²","ç±Œè¨­","é¢¨é›»","é›»åƒ¹","é›»æ¥­","é¦™å¤¾è˜­","è¾²æ¥­è£œåŠ©","CPPA","è¾²é›»","è¾²æ¥­è¨­æ–½è¨±å¯","æ²¼æ°£","ç”Ÿè³ªèƒ½","Solar","PV","energy","solar","storage","å…‰ä¼","èƒ½æºæ”¿ç­–","ç¢³æ¬Š","ç¢³è²»","èº‰è³¼","èƒ½æºç½²","é›»æ¥­æ³•","èº‰è³¼è²»ç‡","æ¼é›»å…±ç”Ÿ"]
            # å…¬å¸æ¸…å–® (å› é•·åº¦é™åˆ¶ï¼Œçœç•¥éƒ¨åˆ†ï¼Œè«‹ç¢ºä¿ä½ å®Œæ•´è²¼ä¸Š)
            company_keywords = ["éº—å‡", "é›²è±¹èƒ½æº", "æ³“å¾·èƒ½æº", "æ£®å´´èƒ½æº", "å°æ±½é›»", "é€²é‡‘ç”Ÿ", "å…ƒæ™¶", "å‹é”"] 
            
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []

            def append_news(title, url, date_obj, source, category):
                if start_date_obj <= date_obj <= end_date_obj:
                    m_title = [k for k in title_keywords if k in title]
                    if m_title:
                        m_comp = [k for k in company_keywords if k in title]
                        dates.append(date_obj.strftime("%Y-%m-%d"))
                        sources.append(source)
                        categories.append(category)
                        title_keyword_matches.append(", ".join(m_title))
                        company_matches.append(", ".join(m_comp) if m_comp else "-")
                        titles.append(title)
                        links.append(url)

            # --- åŸ·è¡Œä½ çš„å„å¤§åª’é«”è¿´åœˆ (Yahoo, UDN, MoneyDJ, è‡ªç”±, ETtoday) ---
            headers = {"User-Agent": "Mozilla/5.0"}
            for kw in keywords:
                # Yahoo
                try:
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={quote(kw)}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    for art in soup.select("li div[class*='Cf']"):
                        a = art.find("a")
                        m = art.find("div", class_="C(#959595)")
                        if a and m:
                            t = a.text.strip()
                            l = a["href"] if a["href"].startswith("http") else f"https://tw.news.yahoo.com{a['href']}"
                            t_str = m.text.strip().split("â€¢")[-1].strip()
                            d_obj = datetime.now()
                            if "å¤©å‰" in t_str: d_obj -= dt.timedelta(days=int(t_str.replace("å¤©å‰","")))
                            elif "å°æ™‚" in t_str or "åˆ†é˜" in t_str: pass
                            else:
                                try: d_obj = datetime.strptime(t_str.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0], "%Y-%m-%d")
                                except: continue
                            append_news(t, l, d_obj, "Yahoo", kw)
                except: continue

                # UDN (ç°¡åŒ–ç‰ˆé‚è¼¯)
                try:
                    res = requests.get(f"https://udn.com/search/word/2/{quote(kw)}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    for h2 in soup.find_all("h2"):
                        a = h2.find("a")
                        if a: append_news(a.text.strip(), a["href"], datetime.now(), "UDN", kw)
                except: continue

            # çµ„åˆæˆ DataFrame ä¸¦å­˜å…¥ session_state
            if titles:
                df = pd.DataFrame({
                    "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                    "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": ""
                }).drop_duplicates(subset=["æ¨™é¡Œ"])
                st.session_state.edited_df = df
                st.success(f"æŠ“å–æˆåŠŸï¼å…± {len(df)} ç­†ã€‚")
            else:
                st.error("æ­¤ç¯„åœå…§æŸ¥ç„¡æ–°èã€‚")

    st.divider()

    # æ­¥é©ŸäºŒï¼šAI æ‘˜è¦ (ä¸²æ¥ä½ åŸæœ¬çš„ OpenAI é‚è¼¯)
    st.header("2ï¸âƒ£ AI è‡ªå‹•æ‘˜è¦")
    if st.button("ğŸ¤– ç”¢ç”Ÿæ‘˜è¦", use_container_width=True):
        if not st.session_state.edited_df.empty:
            client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
            for idx, row in st.session_state.edited_df.iterrows():
                if not row['AI æ–°èæ‘˜è¦']:
                    # é€™è£¡æ˜¯æŠ“å–ç¶²é å…§å®¹ä¸¦æ‘˜è¦çš„é‚è¼¯...
                    st.session_state.edited_df.at[idx, 'AI æ–°èæ‘˜è¦'] = "AI æ‘˜è¦è™•ç†ä¸­..."
            st.rerun()

    st.divider()

    # æ­¥é©Ÿä¸‰ï¼šç™¼ä¿¡
    st.header("3ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ğŸ“§ ä¾ç…§ç›®å‰ç•«é¢ç™¼ä¿¡", use_container_width=True):
        if send_python_email(st.session_state.edited_df):
            st.balloons()
            st.success("éƒµä»¶ç™¼é€æˆåŠŸï¼")

# --- 4. ä¸»ç•«é¢ï¼šç·¨è¼¯æ¸…å–® ---
st.write("### ğŸ“ ç·¨è¼¯ç™¼ä½ˆæ¸…å–®")
st.caption("æç¤ºï¼šé»æ“Šæ¨™é¡Œé€£çµå¯é–‹å•Ÿç¶²é ï¼›é¸å–è¡ŒæŒ‰ Delete å¯åˆªé™¤ã€‚")

if not st.session_state.edited_df.empty:
    # é€™è£¡é¡¯ç¤ºä½ çš„è³‡æ–™ï¼Œä¸¦å°‡ç¶²å€è¨­ç‚ºå¯é»æ“Š
    st.session_state.edited_df = st.data_editor(
        st.session_state.edited_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "æ—¥æœŸ": st.column_config.TextColumn("æ—¥æœŸ", disabled=True),
            "ç¶²å€": st.column_config.LinkColumn("æ¨™é¡Œé€£çµ", width="medium"),
            "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="large"),
            "AI æ–°èæ‘˜è¦": st.column_config.TextColumn("AI æ–°èæ‘˜è¦", width="large")
        },
        column_order=["æ—¥æœŸ", "ä¾†æº", "æ¨™é¡Œ", "ç¶²å€", "AI æ–°èæ‘˜è¦"]
    )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆé¸æ“‡æ—¥æœŸä¸¦åŸ·è¡Œæ­¥é©Ÿä¸€ã€‚")

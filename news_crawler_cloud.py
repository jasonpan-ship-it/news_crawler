import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.header import Header
from email.utils import formataddr
from urllib.parse import quote
import urllib.request as req
import bs4
from datetime import datetime
import datetime as dt
from pandas.tseries.offsets import BusinessDay
import warnings
import time as tt

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# --- 1. ä»‹é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ç¶ èƒ½æ–°èç™¼ä½ˆç³»çµ±", page_icon="âš¡", layout="wide")

if 'edited_df' not in st.session_state:
    st.session_state.edited_df = pd.DataFrame()

# --- 2. å·¥å…·å‡½å¼ ---
def extract_webpage_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in ['article', 'main', 'div']:
            content = soup.find(tag)
            if content and len(content.text.strip()) > 200:
                return content.get_text(separator="\n", strip=True)
        return soup.get_text(separator="\n", strip=True)
    except:
        return ""

def build_html_body(title_text, df, show_company_col=True):
    """
    å»ºç«‹ç¬¦åˆæ‚¨æ ¼å¼è¦æ±‚çš„ HTML è¡¨æ ¼
    show_company_col: æ§åˆ¶æ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€æ¬„ä½
    """
    intro = f"""
    {title_text}<br>
    <p style="color:gray; font-style:italic;">
    (æŠ“å–åŒ…å« <a href="#">ç‰¹å®šé—œéµå­—</a> çš„æ–°èï¼Œå¦‚æœéœ€è¦å¢åŠ æ–°èç¶²ç«™æˆ–é—œéµå­—è«‹è¯ç¹«JP)</p>
    """
    
    html_rows = ""
    for _, row in df.iterrows():
        # æ—¥æœŸæ ¼å¼åŒ–
        try:
            d_str = datetime.strptime(str(row['æ—¥æœŸ']), "%Y-%m-%d").strftime("%m/%d")
        except:
            d_str = str(row['æ—¥æœŸ'])

        # å…¬å¸é—œéµå­—é¡¯ç¤ºè™•ç†
        comp_kw = row.get('åŒ…å«å…¬å¸é—œéµå­—', '-')
        if pd.isna(comp_kw) or comp_kw == "": comp_kw = "-"

        # æ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦ç”¢ç”Ÿå…¬å¸æ¬„ä½çš„ HTML
        company_td = f"<td style='border:1px solid #333; padding:8px;'>{comp_kw}</td>" if show_company_col else ""

        html_rows += f"""
        <tr>
            <td style='border:1px solid #333; padding:8px;'>{d_str}</td>
            <td style='border:1px solid #333; padding:8px;'><a href='{row['ç¶²å€']}'>{row['æ¨™é¡Œ']}</a></td>
            {company_td}
            <td style='border:1px solid #333; padding:8px;'>{row.get('AI æ–°èæ‘˜è¦', '')}</td>
        </tr>"""
    
    # è¡¨é ­è™•ç†ï¼šæ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦é¡¯ç¤ºã€Œå…¬å¸ã€è¡¨é ­
    company_th = '<th style="width:10%;">å…¬å¸</th>' if show_company_col else ''
    
    # èª¿æ•´æ‘˜è¦æ¬„ä½å¯¬åº¦ (å¦‚æœéš±è—å…¬å¸æ¬„ï¼Œæ‘˜è¦æ¬„å¯ä»¥å¯¬ä¸€é»)
    summary_width = "60%" if show_company_col else "70%"

    table_html = f"""
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 14px; border: 1px solid #333;">
        <thead><tr style="background-color: #f2f2f2; text-align: left;">
            <th style="width:5%;">æ—¥æœŸ</th>
            <th style="width:25%;">æ¨™é¡Œ</th>
            {company_th}
            <th style="width:{summary_width};">AIæ‘˜è¦</th>
        </tr></thead>
        <tbody>{html_rows}</tbody>
    </table>
    """
    return f"<html><body>{intro}{table_html}</body></html>"

def send_split_emails(df):
    sender = st.secrets["EMAIL_SENDER"]
    password = st.secrets["EMAIL_PASSWORD"]
    receiver = st.secrets["EMAIL_RECEIVER"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    # è¨­å®šé¡¯ç¤ºåç¨±
    SENDER_NAME = "æ¯æ—¥æ–°èå°å¹«æ‰‹" 
    RECEIVER_NAME = "éº—å‡èƒ½æºé›†åœ˜" 

    # é‚è¼¯ï¼šæœ‰å…¬å¸é—œéµå­— -> Group A (ç«¶æ¥­)ï¼›æ²’æœ‰ -> Group B (ç”¢æ¥­)
    def has_company_kw(val):
        if not val or pd.isna(val): return False
        s = str(val).strip().replace("-", "")
        return len(s) > 0

    group_a = df[df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]
    group_b = df[~df['åŒ…å«å…¬å¸é—œéµå­—'].apply(has_company_kw)]

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, password)
            
            # ç™¼é€ Group A: ç«¶æ¥­æ–°è (é¡¯ç¤ºå…¬å¸æ¬„ä½)
            if not group_a.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç«¶æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=True -> é¡¯ç¤ºå…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç«¶æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_a, show_company_col=True), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç«¶æ¥­æ–°è ({len(group_a)} å°) å·²ç™¼é€")

            # ç™¼é€ Group B: ç”¢æ¥­æ–°è (éš±è—å…¬å¸æ¬„ä½)
            if not group_b.empty:
                msg = MIMEMultipart()
                msg['Subject'] = f"{today_str} ç”¢æ¥­æ–°èæ•´ç†"
                msg['From'] = formataddr((str(Header(SENDER_NAME, 'utf-8')), sender))
                msg['To'] = formataddr((str(Header(RECEIVER_NAME, 'utf-8')), receiver))
                
                # show_company_col=False -> éš±è—å…¬å¸æ¬„ä½
                msg.attach(MIMEText(build_html_body("æœ¬æ—¥ç”¢æ¥­æ–°èæ•´ç†å¦‚ä¸‹ï¼š", group_b, show_company_col=False), 'html'))
                server.send_message(msg)
                st.toast(f"âœ… ç”¢æ¥­æ–°è ({len(group_b)} å°) å·²ç™¼é€")
        return True
    except Exception as e:
        st.error(f"ç™¼ä¿¡å¤±æ•—: {e}")
        return False

# --- 3. å´é‚Šæ¬„ ---
with st.sidebar:
    st.title("âš¡ ç¶ èƒ½æ–°èçˆ¬èŸ²")
    
    st.header("1ï¸âƒ£ æŠ“å–æ–°èè³‡æ–™")
    today_dt = pd.Timestamp.now().normalize()
    last_bus_day = (today_dt - BusinessDay(1)).to_pydatetime()
    s_date = st.date_input("é–‹å§‹æ—¥æœŸ", last_bus_day)
    e_date = st.date_input("çµæŸæ—¥æœŸ", today_dt)
    
    if st.button("ğŸš€ åŸ·è¡Œçˆ¬èŸ²", use_container_width=True):
        with st.spinner("æ­£åœ¨åŠªåŠ›çš„çˆ¬..."):
            start_date_obj = datetime.combine(s_date, datetime.min.time())
            end_date_obj = datetime.combine(e_date, datetime.max.time())
            
            # åˆå§‹åŒ–å„²å­˜åˆ—è¡¨
            dates, sources, categories, company_matches, title_keyword_matches, titles, links = [], [], [], [], [], [], []
            
            # é—œéµå­—è¨­å®š
            keywords = ["å¤ªé™½èƒ½", "å†ç”Ÿèƒ½æº", "é›»å» ", "ç¶ é›»", "å…‰é›»",  "é¢¨é›»", "å„²èƒ½", "ç¶ é›»äº¤æ˜“", "éº—å‡èƒ½æº", "ç¶ èƒ½"]
            
            # (åœ¨æ­¤çœç•¥æ‚¨åŸæœ¬å†—é•·çš„ company_keywords å®šç¾©ï¼Œè«‹ä¿ç•™æ‚¨åŸæœ¬çš„åˆ—è¡¨)
            # å‡è¨­é€™è£¡æœ‰æ‚¨çš„ company_keywords èˆ‡ title_keywords...
            # ç‚ºäº†è®“ç¨‹å¼ç¢¼ç°¡æ½”ï¼Œæˆ‘ç›´æ¥æ²¿ç”¨æ‚¨åŸæœ¬çš„ append_news å‡½å¼é‚è¼¯
            
            def find_company_keywords(text):
                return [k for k in company_keywords if k in text]

            def append_news(title, url, date_obj, source, category):
                if start_date_obj <= date_obj <= end_date_obj:
                    # æª¢æŸ¥æ¨™é¡Œé—œéµå­—
                    matched_title_keywords = [k for k in title_keywords if k in title]
                    if not matched_title_keywords:
                        return
                    
                    # æª¢æŸ¥å…¬å¸é—œéµå­—
                    matched_company_keywords = find_company_keywords(title)
                    
                    dates.append(date_obj.strftime("%Y-%m-%d"))
                    sources.append(source)
                    categories.append(category)
                    title_keyword_matches.append(", ".join(matched_title_keywords))
                    company_matches.append(", ".join(matched_company_keywords) if matched_company_keywords else "-")
                    titles.append(title)
                    links.append(url)

            # --- 1. Yahoo çˆ¬èŸ² (ç¶­æŒåŸæ¨£) ---
            headers = {"User-Agent": "Mozilla/5.0"}
            for kw in keywords:
                try:
                    q = quote(kw)
                    res = requests.get(f"https://tw.news.yahoo.com/search?p={q}", headers=headers)
                    soup = BeautifulSoup(res.text, "html.parser")
                    articles = soup.select("li div[class*='Cf']")
                    for art in articles:
                        a_tag = art.find("a")
                        meta_div = art.find("div", class_="C(#959595)")
                        if not a_tag: continue
                        title = a_tag.text.strip()
                        href = a_tag["href"]
                        full_link = href if href.startswith("http") else f"https://tw.news.yahoo.com{href}"
                        date_obj = None
                        if meta_div:
                            time_str = meta_div.text.strip().split("â€¢")[-1].strip()
                            today = datetime.now()
                            if "å¤©å‰" in time_str:
                                try: date_obj = today - dt.timedelta(days=int(time_str.replace("å¤©å‰", "")))
                                except: pass
                            elif "å°æ™‚å‰" in time_str or "åˆ†é˜å‰" in time_str: date_obj = today
                            elif "å¹´" in time_str:
                                try: date_obj = datetime.strptime(time_str.replace("æ—©ä¸Š","").replace("ä¸‹åˆ","").replace("æ™šä¸Š","").replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","").split()[0], "%Y-%m-%d")
                                except: continue
                        if date_obj: append_news(title, full_link, date_obj, "Yahoo", kw)
                    tt.sleep(0.5)
                except: continue

            # --- 2. UDN çˆ¬èŸ² (ç¶­æŒåŸæ¨£) ---
            for i in range(len(keywords)):
                try:
                    kw = keywords[i]
                    url = f"https://udn.com/search/word/2/{quote(kw)}"
                    req_obj = req.Request(url, headers={"User-Agent": "Mozilla/5.0"})
                    with req.urlopen(req_obj) as response:
                        data = response.read().decode("utf-8")
                    soup = bs4.BeautifulSoup(data, "html.parser")
                    ti_box = soup.find("div", class_="context-box__content story-list__holder story-list__holder--full")
                    if not ti_box: continue
                    ti_h2 = ti_box.find_all("h2")
                    ti_time = ti_box.find_all("time", class_="story-list__time")
                    for l, title_tag in enumerate(ti_h2):
                        a_tag = title_tag.find("a")
                        if not a_tag or l >= len(ti_time): continue
                        title = a_tag.get_text(strip=True)
                        href = a_tag.get("href")
                        try:
                            date_obj = datetime.strptime(ti_time[l].get_text(strip=True)[:10], "%Y-%m-%d")
                            append_news(title, href, date_obj, "UDN", kw)
                        except: continue
                except: continue

            # --- 3. MoneyDJ çˆ¬èŸ² (ç¶­æŒåŸæ¨£) ---
            urls_mdj = [
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?svc=NW&a=X0300023", "èƒ½æº"),
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?index1=2&svc=NW&a=X0300023", "èƒ½æº"),
                ("https://www.moneydj.com/kmdj/common/listnewarticles.aspx?svc=NW&a=C0.C099368", "å¤ªé™½èƒ½")
            ]
            for url, cat in urls_mdj:
                try:
                    req_obj = req.Request(url, headers={"User-Agent": "Mozilla/5.0"})
                    with req.urlopen(req_obj) as response:
                        data = response.read().decode("utf-8")
                    soup = bs4.BeautifulSoup(data, "html.parser")
                    ti = soup.find("div", class_="forumgridBox")
                    if not ti: continue
                    titles7 = ti.find_all("td", class_="ArticleTitle")
                    times7 = ti.find_all("td")
                    base_year = datetime.today().year
                    for i, t_tag in enumerate(titles7):
                        if not t_tag.a: continue
                        href = "https://www.moneydj.com/" + t_tag.a.get("href")
                        title = t_tag.a.text.strip().replace("-MoneyDJç†è²¡ç¶²", "")
                        try:
                            raw_date = times7[i * 3].text.strip()
                            date_obj = datetime.strptime(f"{base_year}/{raw_date}", "%Y/%m/%d")
                            append_news(title, href, date_obj, "MoneyDJ", cat)
                        except: continue
                except: continue

            # --- 4. è‡ªç”±æ™‚å ± (LTN) ä¿®å¾©ç‰ˆ ---
            # å®šç¾© LTN çš„ç›®æ¨™ç¶²å€ (è£œä¸Šé€™æ®µ)
            ltn_urls = [
                ("https://news.ltn.com.tw/topic/å†ç”Ÿèƒ½æº", "å†ç”Ÿèƒ½æº"),
                ("https://news.ltn.com.tw/topic/å¤ªé™½èƒ½", "å¤ªé™½èƒ½"),
                ("https://news.ltn.com.tw/topic/é¢¨åŠ›ç™¼é›»", "é¢¨é›»"),
                ("https://news.ltn.com.tw/topic/ç¶ é›»", "ç¶ é›»"),
            ]

            for url, cat in ltn_urls:
                try:
                    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    # LTN çµæ§‹æ›´æ–°ï¼šå„ªå…ˆæŠ“ ul.searchlist (åˆ—è¡¨é å¸¸è¦‹)ï¼Œå…¶æ¬¡æŠ“ ul.tag_focus
                    items = soup.select("ul.searchlist li") or soup.select("ul.tag_focus li") or soup.select("ul.list li")
                    
                    for item in items:
                        # æ’é™¤å»£å‘Š
                        if "class" in item.attrs and "ad" in item.attrs["class"]:
                            continue

                        # å˜—è©¦æŠ“æ¨™é¡Œ (çµæ§‹å¯èƒ½æ˜¯ h3 æˆ– div.tit)
                        t_tag = item.find("h3") or item.find("div", class_="tit")
                        l_tag = item.find("a")
                        time_tag = item.find("span", class_="time")
                        
                        if t_tag and l_tag:
                            title = t_tag.get_text(strip=True)
                            href = l_tag["href"]
                            if not href.startswith("http"):
                                href = "https://news.ltn.com.tw/" + href.lstrip("/")
                            
                            # æ—¥æœŸè§£æ
                            try:
                                if time_tag:
                                    date_str = time_tag.text.strip().split()[0] # å–å‡º 2025/01/13
                                    date_obj = datetime.strptime(date_str, "%Y/%m/%d")
                                    append_news(title, href, date_obj, "è‡ªç”±æ™‚å ±", cat)
                            except:
                                continue
                except Exception as e:
                    print(f"LTN Error: {e}")

            # --- 5. ETtoday ä¿®å¾©ç‰ˆ ---
            for kw in keywords:
                try:
                    # ä½¿ç”¨ idx=1 å¼·åˆ¶é€²å…¥åˆ—è¡¨æ¨¡å¼
                    u = f"https://www.ettoday.net/news_search/doSearch.php?search_term_string={quote(kw)}&idx=1"
                    res = requests.get(u, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                    soup = BeautifulSoup(res.text, "html.parser")
                    
                    # ETtoday åˆ—è¡¨çµæ§‹é€šå¸¸åœ¨ .archive_list å…§çš„ .box_2
                    # å¿…é ˆç¢ºèª h2 å­˜åœ¨ (æœ‰æ™‚å€™æœƒæœ‰å»£å‘Šæ’åœ¨è£¡é¢)
                    articles = soup.select("div.archive_list div.box_2")
                    
                    for art in articles:
                        h2 = art.find("h2")
                        if not h2 or not h2.find("a"): continue
                        
                        title = h2.find("a").text.strip()
                        href = h2.find("a")["href"]
                        
                        date_tag = art.find("span", class_="date")
                        if date_tag:
                            # æ ¼å¼: "2025/01/13 10:00)" æˆ– "2025/01/13 10:00"
                            try:
                                d_str = date_tag.text.strip()
                                # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æ‹¬è™Ÿæˆ–å¤šé¤˜æ–‡å­—
                                d_str = d_str.replace("(", "").replace(")", "").split(" ")[0] # åªå–æ—¥æœŸéƒ¨åˆ† yyyy/mm/dd
                                date_obj = datetime.strptime(d_str, "%Y/%m/%d")
                                append_news(title, href, date_obj, "ETtoday", kw)
                            except:
                                continue
                except Exception as e:
                    print(f"ETtoday Error: {e}")

            # --- 6. è¡Œæ”¿é™¢å…¬å ± (æš«æ™‚ç•¥é) ---
            # try:
            #     # (åŸä»£ç¢¼å·²è¨»è§£)
            #     pass
            # except Exception as e:
            #     pass

            # --- çµæœå½™æ•´ ---
            if titles:
                df = pd.DataFrame({
                    "æ—¥æœŸ": dates, "ä¾†æº": sources, "åˆ†é¡": categories,
                    "åŒ…å«æ¨™é¡Œé—œéµå­—": title_keyword_matches, "åŒ…å«å…¬å¸é—œéµå­—": company_matches,
                    "æ¨™é¡Œ": titles, "ç¶²å€": links, "AI æ–°èæ‘˜è¦": ""
                }).drop_duplicates(subset=["æ¨™é¡Œ"]).sort_values(by="æ—¥æœŸ", ascending=False).reset_index(drop=True)
                
                # å»ºç«‹éš±è—çš„åŸæ–‡é€£çµæ¬„ä½ä¾› UI é¡¯ç¤º
                df["åŸæ–‡é€£çµ"] = df["ç¶²å€"] 
                st.session_state.edited_df = df
                st.success(f"âœ… æŠ“å–å®Œæˆï¼å…± {len(df)} ç­†æ–°èã€‚")
            else:
                st.error("âŒ æ­¤æ—¥æœŸç¯„åœå…§æŸ¥ç„¡æ–°èã€‚")

    # æ­¥é©ŸäºŒ
    st.header("2ï¸âƒ£ ç”¢ç”ŸAIæ‘˜è¦")
    if st.button("é»æˆ‘", use_container_width=True):
        if not st.session_state.edited_df.empty:
            client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
            for idx, row in st.session_state.edited_df.iterrows():
                if not row['AI æ–°èæ‘˜è¦']:
                    st.write(f"æ‘˜è¦ç”¢ç”Ÿä¸­: {row['æ¨™é¡Œ'][:15]}...")
                    text = extract_webpage_text(row['ç¶²å€'])
                    if text:
                        try:
                            res = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": f"è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ç´„40å€‹å­—ï¼š\n\n{text[:2500]}"}]
                            )
                            st.session_state.edited_df.at[idx, 'AI æ–°èæ‘˜è¦'] = res.choices[0].message.content.strip()
                        except: pass
            st.rerun()

    st.divider()

    # æ­¥é©Ÿä¸‰
    st.header("3ï¸âƒ£ æ­£å¼ç™¼ä¿¡")
    if st.button("ç™¼ä¿¡çµ¦å…¨å…¬å¸", use_container_width=True):
        if not st.session_state.edited_df.empty:
            if send_split_emails(st.session_state.edited_df):
                st.balloons()
                st.success("âœ… æ‰€æœ‰ä¿¡ä»¶ç™¼é€å®Œæˆï¼")
        else:
            st.warning("âš ï¸ ç•«é¢ä¸Šæ²’æœ‰è³‡æ–™ã€‚")

# --- 4. ä¸»ç•«é¢ ---
st.write("### ğŸ“ ç·¨è¼¯ç™¼ä½ˆæ¸…å–®")
st.caption("æç¤ºï¼šé¸å–è¡Œä¸¦æŒ‰ Delete å¯åˆªé™¤ï¼›æ¬„ä½å¯ä¾æ“šç™¼ä¿¡éœ€æ±‚æ‰‹å‹•ä¿®æ”¹ï¼Œæœ‰å…¬å¸é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç«¶æ¥­æ–°èã€ã€æ²’é—œéµå­—çš„æœƒç™¼åœ¨ã€Œç”¢æ¥­æ–°èã€ã€‚")

if not st.session_state.edited_df.empty:
    st.session_state.edited_df = st.data_editor(
        st.session_state.edited_df,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "æ—¥æœŸ": st.column_config.TextColumn("æ—¥æœŸ", disabled=True),
            "æ¨™é¡Œ": st.column_config.TextColumn("æ¨™é¡Œ", width="large"),
            "åŸæ–‡é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="(æŸ¥çœ‹)", width="small"),
            "ç¶²å€": None, # éš±è—åŸå§‹ç¶²å€
            "åŒ…å«å…¬å¸é—œéµå­—": st.column_config.TextColumn("å…¬å¸é—œéµå­—", width="medium"),
            "AI æ–°èæ‘˜è¦": st.column_config.TextColumn("AI æ–°èæ‘˜è¦", width="large")
        },
        column_order=["æ—¥æœŸ", "ä¾†æº", "æ¨™é¡Œ", "åŸæ–‡é€£çµ", "åŒ…å«å…¬å¸é—œéµå­—", "AI æ–°èæ‘˜è¦"]
    )
else:
    st.info("ğŸ‘ˆ è«‹å…ˆå¾å·¦å´åŸ·è¡Œã€Œæ­¥é©Ÿä¸€ã€æŠ“å–æ–°èã€‚")
